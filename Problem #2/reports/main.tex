%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MCM/ICM LaTeX Template %%
%% 2026 MCM/ICM           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{left=1in,right=0.75in,top=1in,bottom=1in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Problem}{F}
\newcommand{\Team}{1111111}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{newtxtext}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{newtxmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\lhead{Team \Team}
\rhead{}
\cfoot{}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xurl}
\usepackage[breaklinks]{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{longtable}
\usepackage[numbers]{natbib}
\let\origthebibliography\thebibliography
\renewcommand{\thebibliography}[1]{\origthebibliography{#1}\sloppy}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}
\sisetup{detect-weight=true, detect-family=true, group-separator={,}, group-minimum-digits=4}

% ---------- Convenience macros ----------
\newcommand{\NetRisk}{\ensuremath{\mathrm{NetRisk}}}
\newcommand{\SubScore}{\ensuremath{\mathrm{Sub}}}
\newcommand{\DefScore}{\ensuremath{\mathrm{Def}}}
\newcommand{\Emp}[1]{E_{#1}} % employment level at year
\newcommand{\gbase}{g_{\mathrm{base}}}
\newcommand{\gadj}{g_{\mathrm{adj}}}

% ---------- Auto-generated numeric macros (from CSV artifacts) ----------
% These are safe to include in the preamble (only \newcommand definitions).
\IfFileExists{tables/scenario_macros.tex}{\input{tables/scenario_macros.tex}}{}

% ---------- Auto-generated tables/figures (from build_report_artifacts.py) ----------
% NOTE: these files contain full LaTeX environments, so they must be \input
% inside the document body (not in the preamble).

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\graphicspath{{.}{figures/}}
\DeclareGraphicsExtensions{.pdf,.jpg,.tif,.png}

% ========== Summary Sheet (first page) ==========
\thispagestyle{empty}
\vspace*{-16ex}
\centerline{\begin{tabular}{*3{c}}
	\parbox[t]{0.3\linewidth}{\begin{center}\textbf{Problem Chosen}\\ \Large \textcolor{red}{\Problem}\end{center}}
	& \parbox[t]{0.3\linewidth}{\begin{center}\textbf{2026\\ MCM/ICM\\ Summary Sheet}\end{center}}
	& \parbox[t]{0.3\linewidth}{\begin{center}\textbf{Team Control Number}\\ \Large \textcolor{red}{\Team}\end{center}}	\\
	\hline
\end{tabular}}

%%%%%%%%%%% Begin Summary %%%%%%%%%%%
\label{page:summary_start}
\begin{itemize}[leftmargin=*, itemsep=0.25em, topsep=0.25em]
  \item \textbf{Careers + institutions.} Software Developers (SDSU), Electricians (LATTC), Writers \& Authors (Academy of Art). National results use employment-weighted SOC bundles (Table~\ref{tab:scenario_summary}).
  \item \textbf{Data \& audit.} BLS OEWS + EP + O*NET; independent reality-check vs AIOE: \(r=\AIOEPearson\), \(\rho=\AIOESpearman\) (Table~\ref{tab:external_benchmark}).
  \item \textbf{Model (1 line).} \(\NetRisk=\frac{\mathrm{Writing}+\mathrm{ToolTech}}{2}-\frac{\mathrm{Physical}+\mathrm{Social}+\mathrm{Creativity}}{3}\); define \(s_{\mathrm{comp}}:=m_i\,s\) with \(m_i\le m_{\max}=0.2\), then \(\gadj=\gbase-s\max(\NetRisk,0)+s_{\mathrm{comp}}\max(-\NetRisk,0)\). \textbf{Projections use calibrated NetRisk when available.}
  \item \textbf{Headline findings (national 2034; SOC bundles).}\IfFileExists{tables/summary_headline_fragment.tex}{\input{tables/summary_headline_fragment.tex}}{ (see Table~\ref{tab:scenario_summary}).}\par
  \item \textbf{Recommendations.} SDSU CS: maintain/slight growth; LATTC Electric: grow capacity; Academy of Art Writing: consolidate/specialize toward higher-originality niches and editing/production workflows (Section~\ref{sec:recommendations}).
  \item \textbf{Limitations.} Scenarios treat \(s\) as a transparent stress-test knob (not a causal estimate); careers are small SOC bundles; local sizing uses scaled openings as a proxy (Section~\ref{sec:robustness}).
\end{itemize}
\vspace{-2pt}
{\scriptsize \ScenarioArtifactsStamp}
\label{page:summary_end}
%%%%%%%%%%% End Summary %%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagestyle{fancy}
\sloppy
\tableofcontents
\newpage
\setcounter{page}{1}
\rhead{Page \thepage\ }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{page:toc_start}

% ============================================================
% Main report
% ============================================================
\section{Problem framing and choices}
\label{sec:framing}
We aim to advise leaders of three post-secondary programs on how to address GenAI, using an auditable model grounded in public labor-market data and an explainable O*NET mechanism layer. Our framing follows tasks-based technology theories: GenAI can substitute for some language/cognitive tasks while complementing others, changing task composition and productivity rather than deterministically eliminating whole occupations \citep{mit_tasks_based_framing, openai_llm_exposure, nber_generative_ai_at_work}. We choose the three careers to match the prompt's STEM/trade/arts categories and to span a wide range of task structures (software = tool/knowledge work; electricians = physical/manual, onsite; writers = writing-intensive creative production).

\section{Data and preprocessing}
\label{sec:data}
\subsection{BLS OEWS (local labor market context)}
OEWS provides local (state and metropolitan) employment and wage levels for occupations. We use these as the institution-specific context inputs: the `local'' employment level and wages for each program's regional labor market \citep{bls_oews_tec}.

\subsection{BLS Employment Projections (national baseline trend)}
EP provides national occupational employment projections over 2024--2034, which we convert to an annual baseline growth rate \(\gbase\). National trajectories use EP (all jobs) for consistency with baseline growth rates; OEWS (wage-and-salary) is used for local labor-market context. We use EP as the no-GenAI baseline trajectory for each focal career (aggregating across the SOCs in its bundle) \citep{bls_ep_table110}.

\subsection{O*NET mechanism layer (why substitution vs.\ complementarity)}
We use O*NET 30.1 `Importance'' ratings from Work Activities, Abilities, and Skills to construct five dimensions. We compute each dimension score per occupation and convert to a percentile across occupations (0--1). This produces interpretable inputs for \(\NetRisk\) \citep{onet_database, onet_taxonomy, onet_license}.

\paragraph{Attribution.} O*NET\textsuperscript{\textregistered} data used under the O*NET Database Content License; see References \citep{onet_license}.

\subsection{Crosswalks and coverage (what gets dropped)}
Occupational taxonomies differ between OEWS/EP (SOC-based) and O*NET (O*NET-SOC). We align them at the SOC occupation code level used in BLS tables by slicing O*NET-SOC codes (e.g., \texttt{15-1252.00}) to their 7-character SOC stem (e.g., \texttt{15-1252}). This merges multiple O*NET-SOC specialties into one SOC occupation, which is appropriate because BLS publishes OEWS/EP at the SOC level.\par
Not every SOC appears in every data source: some SOC codes present in O*NET do not appear in OEWS or EP tables (and vice versa), and some are aggregation/detail differences. Table~\ref{tab:mechanism_coverage} reports counts at each stage (O*NET-SOC $\to$ SOC slice $\to$ relevant elements $\to$ mechanism-scored occupations $\to$ overlap with OEWS/EP) so that any `dropped'' occupations are transparent and auditable.

\IfFileExists{tables/mechanism_coverage.tex}{\input{tables/mechanism_coverage.tex}}{}

\section{Model}
\label{sec:model}
\subsection{Mechanism dimensions}
Let \(d\in\{\mathrm{Writing},\mathrm{ToolTech},\mathrm{Physical},\mathrm{Social},\mathrm{Creativity}\}\) be the five dimensions. For each occupation \(i\), we compute a raw mean importance score from selected O*NET elements, then convert to a percentile across the occupation set:
\[
  x_{i,d} \in [0,1] \quad \text{(percentile among occupations)}.
\]

\subsection{Net Risk index}
Define substitution and defense scores:
\[
  \SubScore_i = \frac{x_{i,\mathrm{Writing}} + x_{i,\mathrm{ToolTech}}}{2}, \qquad
  \DefScore_i = \frac{x_{i,\mathrm{Physical}} + x_{i,\mathrm{Social}} + x_{i,\mathrm{Creativity}}}{3},
\]
and \(\NetRisk_i = \SubScore_i - \DefScore_i\). Positive \(\NetRisk\) implies higher exposure to substitution; negative \(\NetRisk\) implies relative sheltering/complementarity.

Table~\ref{tab:netrisk_summary} provides summary statistics for the \NetRisk{} distribution across all scored occupations, and the interpretation examples below illustrates the interpretation by listing extreme examples from both tails of the distribution.

\IfFileExists{tables/netrisk_summary.tex}{\input{tables/netrisk_summary.tex}}{}
\IfFileExists{tables/netrisk_interpretation.tex}{\input{tables/netrisk_interpretation.tex}}{}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figures/netrisk_hist.png}
\caption{Distribution of NetRisk scores across all occupations in the mechanism layer (from \texttt{data/mechanism\_risk\_scored.csv}).}
\label{fig:netrisk_hist}
\end{figure}

\subsection{External calibration to AI applicability}
To improve rigor, we calibrate the mechanism weights against an external occupation-level AI applicability measure from the \emph{Working with AI} dataset (Tomlinson et al., 2025), which provides SOC-level applicability scores derived from real-world GenAI usage patterns \citep{tomlinson2025working}. We fit nonnegative weights on the five dimensions with defense dimensions entering with negative sign. Because the five dimensions are correlated, the nonnegativity constraint can yield sparse solutions (some weights at or near zero), which should be interpreted as ``adds little predictive power given the other dimensions'' rather than ``dimension is irrelevant.''\par
In other words, multiple dimensions can be jointly informative but redundant: when two percentile features move together across occupations, the constrained fit may assign weight primarily to one and drive the other toward zero without meaning that the underlying mechanism is absent. To guard against over-interpreting a single fitted weight vector, we include a simple weight-robustness check (Table~\ref{tab:weight_sensitivity}).\par
To reconcile interpretability vs.\ prediction, we explicitly report \textbf{two} indices: (i) the equal-weight mechanism index \(\NetRisk_{\text{uncal}}\) (used for the substitution/defense narrative), and (ii) the calibrated predictive index \(\NetRisk_{\text{cal}}\) (used for scenario projections when available). Tables~\ref{tab:netrisk_index_compare}--\ref{tab:netrisk_index_disagree} quantify how closely these indices align and show concrete cases where they differ.\par
The calibrated weights define \(\NetRisk_{\text{cal}}\) by taking the signed weighted sum, centering it to mean zero across occupations, and rescaling to approximately \([-1,1]\). When calibration outputs are available, the pipeline uses \(\NetRisk_{\text{cal}}\) in downstream scenario projections; otherwise it uses \(\NetRisk_{\text{uncal}}\).

\IfFileExists{tables/calibration_summary.tex}{\input{tables/calibration_summary.tex}}{}
\IfFileExists{tables/netrisk_index_compare.tex}{\input{tables/netrisk_index_compare.tex}}{}
\IfFileExists{tables/netrisk_index_disagree.tex}{\input{tables/netrisk_index_disagree.tex}}{}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{figures/calibration_scatter.png}
\caption{Calibration fit: observed vs.\ predicted AI applicability (from \texttt{data/calibration\_fit.csv}).}
\label{fig:calibration_scatter}
\end{figure}

\subsection{Scenario employment projection}
Let \(\Emp{2024}\) be the 2024 employment level used for the projection base (from BLS EP for national scenarios) and \(\gbase\) the EP annual baseline growth. OEWS is used for \emph{local} labor market context and program sizing (Section~\ref{sec:recommendations}). For a scenario parameter \(s\ge 0\), define adjusted growth:
\[
  \gadj = \begin{cases} 
    \gbase - s \cdot \NetRisk & \text{if } \NetRisk \ge 0 \\
    \gbase + (\,m_i\, s\,)\cdot(-\NetRisk) & \text{if } \NetRisk < 0 
  \end{cases}
\]
\begin{center}
\fbox{\begin{minipage}{0.97\textwidth}
\small
\textbf{Minimal microfoundation (task share $\rightarrow$ productivity $\rightarrow$ labor demand).} Interpret our substitution score as an \emph{AI-exposed task share} \(\tau_i=\SubScore_i\in[0,1]\). Let \(A\in[0,1]\) be adoption intensity by 2034 and \(r\in[0,1]\) be the fraction of exposed tasks that become effectively automatable/time-saving. A reduced-form productivity shift is
\[
  \Delta \ln \mathrm{Prod}_i \approx A\,r\,\tau_i.
\]
If cost reductions pass through and output demand has elasticity \(\varepsilon_i\), then (linearizing) employment responds as
\[
  \Delta \ln L_i \approx (\varepsilon_i-1)\,\Delta \ln \mathrm{Prod}_i \approx (\varepsilon_i-1)\,A\,r\,\tau_i.
\]
This yields an annualized growth headwind/uplift over a decade:
\[
  \Delta g_i \approx \frac{(\,1-\varepsilon_i\,)\,A\,r}{10}\;\tau_i \quad \Rightarrow \quad s \approx \frac{(\,1-\varepsilon\,)\,A\,r}{10},
\]
so \(s\) has a concrete interpretation as an \emph{adoption $\times$ automability $\times$ elasticity-wedge} parameter (Table~\ref{tab:scenario_params}). Our \(\NetRisk_i=\SubScore_i-\DefScore_i\) then acts as the signed shifter: defense raises the extent to which productivity gains translate into complementarity rather than displacement.\par
\textbf{Numeric anchor matching Moderate/High.} Empirically, \(p90(\tau \mid \NetRisk>0)\approx 0.86\) in our scored mechanism layer. Using \(A=0.6\), \(r=0.3\), and \(\tau_{p90}\approx 0.86\), we get \(A r \tau \approx 0.155\), which annualizes to \(\approx 1.5\%\)/yr---matching the Moderate reference point. Increasing \(A\) and/or \(r\) yields \(\approx 3\%\)/yr for High.
\end{minipage}}
\end{center}
We use the microfoundation to interpret \(s\) (units and magnitude), while \(\NetRisk\) supplies the signed direction (substitution vs.\ defense) in a reduced-form scenario mapping.\par
We interpret complementarity uplift as demand-side growth that is \emph{bounded} by (i) how hard it is to scale quantity (physical/onsite constraints, credentialing) and (ii) effective demand elasticity. Concretely, we define an occupation-level multiplier
\[
  m_i=\min\{m_{\max},(1-B_i)\tilde{\varepsilon}_i\},
\]
where \(B_i\in[0,1]\) is a bottleneck index (computed from our existing Physical percentile plus a coarse licensing proxy by SOC major group), and \(\tilde{\varepsilon}_i\in[0,1]\) is a normalized demand-responsiveness proxy (not a literal elasticity; binned by career type / SOC major group).\par
\textbf{Interpretation.} We treat \(\tilde{\varepsilon}_i\) as a conservative \textbf{rank-based demand-responsiveness proxy} (not an elasticity); sensitivity checks (Tables~\ref{tab:comp_cap_components} and \ref{tab:comp_factor_sensitivity}) show conclusions are unchanged under reasonable variation.\par
\IfFileExists{tables/comp_cap_components.tex}{\input{tables/comp_cap_components.tex}}{}
\textbf{Numeric anchor for \(m_{\max}=0.2\).} We choose \(m_{\max}\) so that even under High disruption, the implied \emph{maximum} annual uplift for a strongly sheltered occupation remains well below 1\%/yr. For example, for Electricians (bundle \(\NetRisk\approx-0.888\)) under High (\(s_{\text{High}}=0.0375\); Table~\ref{tab:scenario_params}), the uplift term is bounded by
\[
  \Delta g_{\text{uplift}} \le m_{\max}\,s_{\text{High}}\,|\NetRisk|
  = 0.2 \cdot 0.0375 \cdot 0.888 \approx 0.0067 \quad (\approx 0.67\%\text{/yr}),
\]
which cumulates to \(\lesssim 6.8\%\) extra employment over 10 years. In our base bottleneck/elasticity construction, Electricians have \(m_{\mathrm{eff}}\approx 0.01\) (Table~\ref{tab:comp_cap_components}), so the realized uplift is far smaller than the 0.67\%/yr upper bound. Thus, even at High disruption, complementarity can add at most sub-1\% annual growth for strongly sheltered occupations, preventing implausible booms from the uplift channel alone. Table~\ref{tab:comp_factor_sensitivity} shows sensitivity to the cap.\par
Why calibrate to the \(p90\) tail? Anchoring Moderate/High at \(p90(\NetRisk_+)\) (instead of the maximum) targets ``highly exposed but not extreme'' occupations, yielding a stable reference point that is less sensitive to outliers and measurement noise while still representing the upper-risk tail.\par
Scenario strengths are defined by this reference-point calibration: we target a growth headwind of 1.5\% (Moderate) or 3\% (High) for the 90th percentile of the \emph{positive} \(\NetRisk\) distribution. Concretely, the pipeline computes \(p90(\NetRisk_+)\) from the calibrated mechanism layer, then sets
\[
  s_{\text{Moderate}} = 0.015 / p90(\NetRisk_+), \qquad s_{\text{High}} = 0.03 / p90(\NetRisk_+),
\]
so that the 90th-percentile exposed occupation experiences the intended annual headwind under the piecewise mapping. Table~\ref{tab:scenario_params} reports the scenario parameters actually used (including whether they came from calibration outputs or defaults). We project 2034 employment as \(\Emp{2034} = \Emp{2024}(1+\gadj)^{10}\). We treat \(s\) as a scenario knob rather than an estimated causal effect; Section~\ref{sec:robustness} reports a sensitivity grid.

\IfFileExists{tables/scenario_params.tex}{\input{tables/scenario_params.tex}}{}

\subsection{Definitions \& Provenance (auditability)}
\label{sec:definitions_provenance}
\begin{center}
\fbox{\begin{minipage}{0.97\textwidth}
\small
\textbf{Two NetRisk indices (reported side-by-side).}
\begin{itemize}[leftmargin=*]
  \item \textbf{NetRisk (uncalibrated; mechanism/interpretability index).} Defined from O*NET percentile scores:
  \[
    \NetRisk_{\text{uncal}}=\underbrace{\frac{\mathrm{Writing}+\mathrm{ToolTech}}{2}}_{\SubScore}-\underbrace{\frac{\mathrm{Physical}+\mathrm{Social}+\mathrm{Creativity}}{3}}_{\DefScore}.
  \]
  This equal-weight form is used to \emph{explain} substitution vs.\ defense mechanisms (what drives risk).
  \item \textbf{NetRisk (calibrated; predictive index).} A signed weighted sum calibrated to an external occupation-level AI applicability score (Tomlinson et al., 2025), then centered and rescaled to approximately \([-1,1]\). Because the O*NET dimensions are correlated and weights are constrained nonnegative, calibration can push one of a correlated pair (e.g., Writing vs.\ Tool/Tech) toward zero without implying the underlying mechanism is absent.
  \item \textbf{Which index drives scenarios?} Scenario projections use \textbf{calibrated NetRisk when available}; otherwise they fall back to the uncalibrated index. See Tables~\ref{tab:netrisk_index_compare}--\ref{tab:netrisk_index_disagree} for correlation and disagreement examples.
\end{itemize}

\textbf{What drives scenarios (national 2024--2034).}
\begin{itemize}[leftmargin=*]
  \item \textbf{Baseline trajectory.} $E_{2024}$ and baseline growth $\gbase$ come from BLS Employment Projections (EP). Baseline 2034 is $E_{2034}=E_{2024}(1+\gbase)^{10}$.
  \item \textbf{Disruption mapping.} We adjust growth with a transparent scenario knob $s$:
  \[
    \gadj = \gbase - s\cdot\max(\NetRisk,0) + (\,m_i\, s\,)\cdot\max(-\NetRisk,0), \qquad m_i \le m_{\max}=0.2,
  \]
  and project $E_{2034}=E_{2024}(1+\gadj)^{10}$. \textbf{Ramp} scenarios linearly increase $s(t)$ from 0 (2024) to $s$ (2034).
  \item \textbf{How Moderate/High are set.} When calibration outputs exist, we set $s$ so that the 90th percentile of the positive NetRisk tail experiences a $1.5\%$ (Moderate) or $3.0\%$ (High) annual growth headwind; the exact values used are reported in Table~\ref{tab:scenario_params}.
\end{itemize}

\textbf{What a “career bundle” contains (SOC occupations; EP employment weights).}
\footnotesize
\IfFileExists{tables/bundle_contents_fragment.tex}{\input{tables/bundle_contents_fragment.tex}}{}
\end{minipage}}
\end{center}

\section{Results}
\label{sec:results}
\subsection{National outcomes for the three careers}
Each career is represented by a small bundle of SOC occupations (e.g., 2--3 codes per career); outcomes are employment-weighted over the bundle so that larger occupations contribute more to the career-level projection. Table~\ref{tab:scenario_summary} reports baseline vs.\ disruption outcomes and the \(\NetRisk\) range (min--max across the bundle) as a robustness check.

\IfFileExists{tables/scenario_summary.tex}{\input{tables/scenario_summary.tex}}{}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figures/scenario_bar.png}
\caption{2034 employment for the three careers under baseline, moderate, and high disruption (generated from \texttt{data/scenario\_summary.csv}).}
\label{fig:scenario_bar}
\end{figure}

\subsection{Dynamic Adoption}
The scenarios above assume immediate adoption of GenAI at full impact. In reality, adoption may ramp gradually over the decade. Table~\ref{tab:scenario_summary} includes `Ramp Moderate'' and `Ramp High'' scenarios that model gradual adoption, where disruption effects increase linearly from zero in 2024 to full scenario strength by 2034. Across all three careers, the ramp outcomes lie between the baseline and immediate-disruption outcomes reported in Table~\ref{tab:scenario_summary}, consistent with a gradual diffusion of GenAI impacts over time. These ramp scenarios suggest that institutions have a window to adapt curricula and program structures as adoption accelerates, rather than facing immediate disruption.

\subsection{Job openings and program sizing}
Beyond net employment growth, annual job openings drive training demand. Table~\ref{tab:openings_summary} reports annual openings from EP projections and their implications for program sizing decisions.

\IfFileExists{tables/openings_summary.tex}{\input{tables/openings_summary.tex}}{}

\subsection{Sensitivity and sanity checks}
\label{sec:robustness}
We conduct extensive robustness checks to validate the stability of our model components.

\subsubsection{Mechanism Layer Stability}
The O*NET mechanism layer relies on specific descriptor choices and normalization methods. To test structural stability, we perturbed the descriptor set (leave-one-out) and tested alternative normalization (percentiles, z-score mapped to \([0,1]\) via Normal CDF, min--max scaling, and within-major-group ranks). Table~\ref{tab:mechanism_sensitivity} summarizes the results: the sign of \(\NetRisk\) for our three focal careers remains stable across perturbations, confirming that the classification of careers as ``exposed'' vs.\ ``sheltered'' is not an artifact of specific element selection.

\IfFileExists{tables/mechanism_sensitivity.tex}{\input{tables/mechanism_sensitivity.tex}}{}

\subsubsection{Calibration Validation}
We validate the calibration weights using 5-fold cross-validation and bootstrap resampling (Table~\ref{tab:calibration_validation}). The model achieves stable out-of-sample performance, and bootstrap analysis confirms that the negative weights on defense dimensions are statistically robust. We also compare the calibrated index against an uncalibrated (equal-weight) baseline; the ranking of focal careers is preserved, though the magnitude of separation increases under calibration.

\IfFileExists{tables/calibration_validation.tex}{\input{tables/calibration_validation.tex}}{}

\subsubsection{External benchmark reality check}
As an additional independent check, we compare our NetRisk indices against a published occupation-level AI exposure measure: the AI Occupational Exposure (AIOE) dataset of \citet{felten2021aioe}. We report correlation of both the uncalibrated mechanism index and the calibrated predictive index with AIOE (Table~\ref{tab:external_benchmark}), and show a few large-disagreement examples to guide interpretation (Table~\ref{tab:external_benchmark_disagree}). Because these measures were constructed with different assumptions and targets, perfect agreement is not expected; we use this check to ensure our ranking is broadly plausible and to transparently flag where our mechanism differs from a widely-cited benchmark.

\IfFileExists{tables/external_benchmark.tex}{\input{tables/external_benchmark.tex}}{}
\IfFileExists{tables/external_benchmark_disagree.tex}{\input{tables/external_benchmark_disagree.tex}}{}

\subsubsection{Scenario and Parameter Sensitivity}
We include three compact robustness checks for the scenario projections: (i) a sensitivity grid over plausible \(s\) values (Table~\ref{tab:sensitivity_grid}); (ii) a sanity-check table listing the most exposed and most sheltered occupations by \(\NetRisk\) in the full scored set (Table~\ref{tab:top_exposed_sheltered}); and (iii) Monte Carlo uncertainty intervals for 2034 employment under Moderate and High disruption (Table~\ref{tab:uncertainty_summary}).
These checks highlight that while the exact 2034 employment level depends on \(s\), the \emph{direction} of impact is structurally determined by the mechanism layer.

\IfFileExists{tables/sensitivity_grid.tex}{\input{tables/sensitivity_grid.tex}}{}
\IfFileExists{tables/comp_factor_sensitivity.tex}{\input{tables/comp_factor_sensitivity.tex}}{}
\IfFileExists{tables/weight_sensitivity.tex}{\input{tables/weight_sensitivity.tex}}{}
\IfFileExists{tables/top_exposed_sheltered.tex}{\input{tables/top_exposed_sheltered.tex}}{}
\IfFileExists{tables/uncertainty_summary.tex}{\input{tables/uncertainty_summary.tex}}{}

\subsubsection{Local openings scaling robustness}
Local program sizing requires scaling national EP annual openings to a metro/state labor market. Because location quotients (LQ) already encode relative local concentration, multiplying both a local occupation-share factor and LQ can double-count concentration. We therefore use the standard decomposition (local total employment share $\times$ LQ); when LQ is computed from the same OEWS employment counts, this is algebraically equivalent to scaling by local occupation share alone. We apply a conservative LQ clip as a stress-test and report a robustness comparison across scaling rules in Table~\ref{tab:openings_scaling_robustness}.

\IfFileExists{tables/openings_scaling_robustness.tex}{\input{tables/openings_scaling_robustness.tex}}{}

\section{Institution-specific recommendations}
\label{sec:recommendations}
Recommendations are organized to answer the prompt: (i) whether to grow or shrink program size and how, and (ii) what to teach about GenAI to best support employability, tied back to model outputs and local context.

Table~\ref{tab:local_context} provides local labor market context including location quotients (LQ). We define an auxiliary \textit{Attractiveness Score} to inform positioning:
\[ \text{Attractiveness} = 0.4 \cdot (\text{Wage Premium}) + 0.3 \cdot (\text{Normalized Local Emp}) + 0.3 \cdot (\text{Normalized LQ}) \]
where normalized employment and LQ are min-max scaled across the three institutions. Table~\ref{tab:program_sizing} provides quantitative guidance on program sizing (annual intake) derived from estimated local annual openings. Local openings are computed from national EP openings using a standard decomposition (local total employment share $\times$ LQ, with conservative clipping); Table~\ref{tab:openings_scaling_robustness} shows a robustness comparison to alternative scaling rules. To account for uncertainty in program efficiency (completion rates and placement rates), we report recommended intake ranges rather than single point estimates.
Why target 5\% / 10\% / 15\% of local openings? These shares are not claims about market power; they are \emph{planning heuristics} spanning realistic capacity regimes: 5\% represents a conservative seat target for resource-constrained programs, 10\% is an aspirational ``steady-state'' target for a strong regional program with sustained placement partnerships, and 15\% represents an aggressive expansion scenario (e.g., additional faculty/labs, expanded apprenticeships) that may be feasible for high-demand fields. The resulting seat ranges translate openings into intake while explicitly accounting for completion$\times$placement uncertainty.

\IfFileExists{tables/local_context.tex}{\input{tables/local_context.tex}}{}
\IfFileExists{tables/program_sizing.tex}{\input{tables/program_sizing.tex}}{}

\subsection{SDSU (Software Developers)}
\textbf{Program size.} Maintain or modestly grow cohorts; disruption primarily reduces growth rate rather than reversing demand (Table~\ref{tab:scenario_summary}).\par
\textbf{Curriculum.} Shift emphasis from boilerplate coding to system design, testing, security, and AI-assisted development with audit trails; make students fluent in evaluating and verifying model outputs.\par
\textbf{Policy.} Permit GenAI use in advanced courses with required disclosure and reproducibility; constrain use in early courses to ensure fundamentals.
\par\textbf{Concrete actions (measurable).}
\begin{itemize}[leftmargin=*]
  \item \textbf{AI-assisted software engineering rubric.} Require every capstone/upper-division project to include (i) tests and CI, (ii) a model-output verification checklist, and (iii) an AI usage disclosure appendix.\newline
  \emph{Metric:} \% of submissions with passing test suites; defect rate in instructor code review; disclosure compliance rate.
  \item \textbf{Model evaluation and security module.} Add a short required module on prompt injection, data leakage, licensing/IP, and evaluation design.\newline
  \emph{Metric:} performance on a standardized red-team + verification assessment (before/after module).
  \item \textbf{Assessment design resilient to GenAI.} Increase oral defenses and timed debugging tasks in core courses.\newline
  \emph{Metric:} gap between in-person performance and take-home performance (reduced variance indicates more robust assessment).
\end{itemize}

\subsection{LATTC (Electricians)}
\textbf{Program size.} Grow capacity and apprenticeship pathways; the career is sheltered by high physical/manual defense and remains strong across scenarios (Table~\ref{tab:scenario_summary}).\par
\textbf{Curriculum.} Double down on hands-on competencies while adding `AI as a tool'' modules for diagnostics, scheduling, documentation, and code-compliant planning.\par
\textbf{Policy.} Teach safe, privacy-preserving, low-compute uses (templates, checklists) appropriate for small contractors.
\par\textbf{Concrete actions (measurable).}
\begin{itemize}[leftmargin=*]
  \item \textbf{AI-assisted job documentation.} Require students to produce work orders, inspection-ready notes, and material lists using structured templates (GenAI optional) with verification against NEC/local code excerpts.\newline
  \emph{Metric:} documentation completeness score; code-compliance error rate on practical exams.
  \item \textbf{Diagnostic reasoning labs.} Use fault-tree exercises where students must justify each step (GenAI permitted as a tutor, not as an answer key).\newline
  \emph{Metric:} time-to-diagnosis and accuracy on standardized fault scenarios; safety-critical mistake rate.
  \item \textbf{Apprenticeship alignment.} Expand employer partnerships to target annual seats suggested by local openings (Table~\ref{tab:program_sizing}).\newline
  \emph{Metric:} placement rate into apprenticeships; employer satisfaction survey on graduates' documentation and troubleshooting skills.
\end{itemize}

\subsection{Academy of Art University (Writers and Authors)}
\textbf{Program size.} Consolidate and specialize toward higher-originality work and editing/production roles; high disruption can flip the field to contraction (Table~\ref{tab:scenario_summary}).\par
\textbf{Curriculum.} Emphasize narrative strategy, editing, and provenance-aware workflows. Teach students to use GenAI as a draft accelerator while differentiating through voice, revision quality, and IP-aware sourcing.\par
\textbf{Policy.} Require disclosure and provenance in portfolios; adopt rubrics that reward originality and documented creative process.
\par\textbf{Concrete actions (measurable).}
\begin{itemize}[leftmargin=*]
  \item \textbf{Portfolio provenance standard.} Require every portfolio piece to include a process log (outline $\to$ drafts $\to$ revisions) and a disclosure statement for any tool-assisted content.\newline
  \emph{Metric:} \% of portfolio pieces with complete provenance; rubric scores on originality/voice and revision quality.
  \item \textbf{Editing and production track.} Create an explicit concentration in editing, story development, and content production workflows where human judgment is primary.\newline
  \emph{Metric:} internship/placement share into editing, producer-assistant, UX/technical writing, or content operations roles.
  \item \textbf{Assessment designed for authenticity.} Use in-class writing sprints and oral defenses (students explain intent, sources, and revision decisions).\newline
  \emph{Metric:} integrity-incident rate; inter-rater reliability of originality scoring.
\end{itemize}

\subsubsection{Transition Plan}
For students currently enrolled in the Writers and Authors program, we recommend redirecting to absorber programs with lower \NetRisk{} due to higher tool complementarity and stronger defense dimensions. Specific transition pathways include:

\begin{itemize}[leftmargin=*]
  \item \textbf{UX Writing.} Redirect students toward user experience writing programs, which exhibit lower \NetRisk{} due to higher \DefScore{} components: elevated Social dimension (user research, cross-functional collaboration) and Creativity dimension (design thinking, user-centered storytelling). The Writing component remains relevant but is complemented by collaborative and research-intensive workflows that GenAI augments rather than substitutes.
  
  \item \textbf{Technical Communication.} Transition students to technical communication programs, which show reduced \NetRisk{} through higher Social dimension scores (stakeholder communication, documentation for diverse audiences) and ToolTech complementarity (GenAI assists in documentation generation while human expertise ensures accuracy, clarity, and domain-specific nuance). The combination of social coordination and tool-assisted workflows creates a complementary rather than substitutive dynamic.
  
  \item \textbf{Digital Media Production.} Redirect toward digital media production programs, which demonstrate lower \NetRisk{} via elevated Physical dimension (hands-on production work, equipment operation) and Creativity dimension (multimedia storytelling, visual narrative). The physical and creative defense dimensions provide sheltering that pure writing-intensive programs lack, while maintaining narrative and content creation skills.
\end{itemize}

These absorber programs are justified by the model: each exhibits a \NetRisk{} profile more favorable than Writers and Authors due to higher \DefScore{} (particularly Social and Creativity dimensions) relative to \SubScore{}, indicating that GenAI serves as a complementary tool rather than a direct substitute for core competencies.

\section{Beyond employability: other success metrics}
\label{sec:other_factors}
Employability is necessary but not sufficient. We propose additional success metrics the prompt highlights: learning integrity and attribution compliance, equity/access, and sustainability (energy/water/compute cost). 

We formalize the trade-offs with a robust rule-based decision model that maps institution-specific constraints to three policy regimes: \textit{Ban} (restrict use in assessment), \textit{Allow-with-Audit} (permit with strict provenance), and \textit{Require} (integrate into workflow).
The rules consider:
\begin{itemize}
    \item \textbf{Exposure Risk:} If \(\NetRisk > 0\), GenAI is a substitute that threatens skill acquisition. This requires either \textit{Ban} (if audit capacity is low) or \textit{Allow-with-Audit} (if audit capacity is high).
    \item \textbf{Sheltering:} If \(\NetRisk < 0\), GenAI is a complement. We recommend \textit{Require} to capture productivity, unless sustainability is the dominant constraint (in which case \textit{Ban} or limit usage to save compute).
\end{itemize}

Table~\ref{tab:policy_regimes} summarizes the policy requirements. Table~\ref{tab:policy_decision} reports the resulting recommended policy per institution under alternative weight regimes (Balanced, Integrity-First, Sustainability-First). We test these rules against perturbations in risk scores and audit capacity estimates; Table~\ref{tab:policy_sensitivity} reports the stability of these recommendations.

\IfFileExists{tables/policy_regimes.tex}{\input{tables/policy_regimes.tex}}{}
\IfFileExists{tables/policy_decision.tex}{\input{tables/policy_decision.tex}}{}
\IfFileExists{tables/policy_sensitivity.tex}{\input{tables/policy_sensitivity.tex}}{}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\linewidth]{figures/policy_tradeoff.png}
\caption{Policy trade-offs under balanced weights (generated from \texttt{data/policy\_decision\_scores.csv}).}
\label{fig:policy_tradeoff}
\end{figure}

\section{Generalization}
\label{sec:generalization}
The mechanism layer and scenario framework generalize to other programs by: (i) swapping the occupation(s), (ii) recomputing local OEWS context for the institution's region, and (iii) choosing scenario parameters \(s\) appropriate for the institution's risk tolerance. Institution-specific recommendations vary primarily through local labor market demand, program mission, and constraints (e.g., resources, accreditation, student population).

\section{Prompt coverage checklist}
\label{sec:checklist}
\begin{itemize}[leftmargin=*]
  \item \textbf{Grow/shrink programs and transitions:} Section~\ref{sec:recommendations}.
  \item \textbf{What to teach about GenAI (including energy/water + attribution):} Sections~\ref{sec:recommendations} and \ref{sec:other_factors}.
  \item \textbf{Other success metrics beyond employment and how recs change:} Section~\ref{sec:other_factors}.
  \item \textbf{Generalization beyond one institution/program:} Section~\ref{sec:generalization}.
\end{itemize}

\newpage
\section*{References}
\bibliographystyle{plainnat}
\bibliography{references}

% AI Use Report (does not count toward 25 pages)
\clearpage
\appendix
\section{Mechanism Layer Details}
\label{app:mechanism}
Table~\ref{tab:onet_elements_appendix} lists the specific O*NET elements (Importance scale) selected for each mechanism dimension.

\IfFileExists{tables/onet_elements_appendix.tex}{\input{tables/onet_elements_appendix.tex}}{}

\section*{AI Use Report}
\input{ai_use_report.tex}

\end{document}
