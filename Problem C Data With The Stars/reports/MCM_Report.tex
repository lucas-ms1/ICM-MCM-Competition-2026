% ============================================================
% MCM 2026 Problem C — Data With The Stars
% Full submission: Summary Sheet + TOC + Report + Memo + Refs + AI Use
% Artifacts: reports/tables/*.csv, reports/tables/judges_save_alpha.json
% ============================================================
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

% ---------- Convenience macros ----------
\newcommand{\DWTS}{\textit{Dancing with the Stars}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}

% ---------- Auto-generated parameter macros (from pipeline artifacts) ----------
% main.py writes reports/tables/judges_save_alpha.tex so the PDF matches judges_save_alpha.json.
\IfFileExists{tables/judges_save_alpha.tex}{\input{tables/judges_save_alpha.tex}}{
  \providecommand{\AlphaJudgesSave}{0.0000}
  \providecommand{\AlphaJudgesSaveNObs}{0}
  \providecommand{\AlphaJudgesSaveNegLogLik}{0.00}
}
% main.py writes reports/tables/fit_diagnostics_summary.tex so fit metrics match fit_diagnostics.csv.
\IfFileExists{tables/fit_diagnostics_summary.tex}{\input{tables/fit_diagnostics_summary.tex}}{
  \providecommand{\FitDiagNWeeks}{0}
  \providecommand{\FitDiagMeanLogPModel}{0.000}
  \providecommand{\FitDiagMeanLogPJudgesOnly}{0.000}
  \providecommand{\FitDiagMeanLogPRandom}{0.000}
  \providecommand{\FitDiagMAPMatchModelPct}{0.0}
  \providecommand{\FitDiagMAPMatchJudgesOnlyPct}{0.0}
  \providecommand{\FitDiagTrueRankMean}{0.00}
  \providecommand{\FitDiagTrueRankLEOnePct}{0.0}
  \providecommand{\FitDiagTrueRankLETwoPct}{0.0}
  \providecommand{\FitDiagTrueRankLEThreePct}{0.0}
  \providecommand{\FitDiagNPercent}{0}
  \providecommand{\FitDiagMAPMatchPercentModelPct}{0.0}
  \providecommand{\FitDiagMAPMatchPercentJudgesOnlyPct}{0.0}
  \providecommand{\FitDiagMeanLogPPercentModel}{0.000}
  \providecommand{\FitDiagNRank}{0}
  \providecommand{\FitDiagMAPMatchRankModelPct}{0.0}
  \providecommand{\FitDiagMAPMatchRankJudgesOnlyPct}{0.0}
  \providecommand{\FitDiagMeanLogPRankModel}{0.000}
}
% main.py writes reports/tables/fan_shares_bootstrap_summary.tex for uncertainty summaries.
\IfFileExists{tables/fan_shares_bootstrap_summary.tex}{\input{tables/fan_shares_bootstrap_summary.tex}}{
  \providecommand{\BootstrapNCells}{0}
  \providecommand{\BootstrapWidthMedian}{0.0000}
  \providecommand{\BootstrapWidthQOne}{0.0000}
  \providecommand{\BootstrapWidthQThree}{0.0000}
  \providecommand{\BootstrapWidthP95}{0.0000}
}
% main.py writes reports/tables/robustness_summary.tex for robustness statistics.
\IfFileExists{tables/robustness_summary.tex}{\input{tables/robustness_summary.tex}}{
  \providecommand{\RobustNWeeks}{0}
  \providecommand{\RobustZeroCount}{0}
  \providecommand{\RobustZeroPct}{0.0}
  \providecommand{\RobustInfCount}{0}
  \providecommand{\RobustInfPct}{0.0}
  \providecommand{\RobustMedian}{0.00}
  \providecommand{\RobustQOne}{0.00}
  \providecommand{\RobustQThree}{0.00}
  \providecommand{\RobustMax}{0.00}
}

\title{MCM 2026 Problem C: Data With The Stars}
\author{Team \#\_\_\_\_}
\date{}

\begin{document}
\maketitle

% ============================================================
% 1) One-page Summary Sheet
% ============================================================
\section*{Summary Sheet}
\begin{itemize}[leftmargin=*]
  \item \textbf{Goal.} Infer weekly fan vote \emph{shares} (latent) from judges' scores and elimination outcomes; compare rank vs.\ percent vote-combination rules; quantify drivers of judges vs.\ fans; propose and evaluate a better system.
  \item \textbf{Key modeling idea.} Fan shares are generated by a latent preference model
  \[
    f_{i,t} = \frac{\exp(u_{i,t})}{\sum_{k\in A_t}\exp(u_{k,t})},\quad
    u_{i,t}=\beta_0+\beta_J\,\tilde J_{i,t}+\beta_P\,\text{momentum}_{i,t}+\beta_U\,\text{underdog}_{i,t}+\beta_X^\top X_i,
  \]
  where $\tilde J$ is a normalized judges score and $X_i$ are celebrity/pro covariates.
  \item \textbf{Estimation.} $\beta$ and $\tau$ are fit by maximum likelihood so that implied eliminations (and finals ordering) align with observed outcomes under the season-appropriate rule (rank vs.\ percent). The pipeline \emph{fails fast} if the optimizer does not converge; all reported numbers come from the run that produced \texttt{fitted\_params.json}.
  \item \textbf{Consistency (fit to eliminations).} Using the fitted latent fan-share model, the implied (MAP) eliminated contestant matches the observed in \textbf{\FitDiagMAPMatchPercentModelPct\%} of percent-rule weeks ($\FitDiagNPercent$) and \textbf{\FitDiagMAPMatchRankModelPct\%} of rank-rule weeks ($\FitDiagNRank$). Proper scoring: mean log probability assigned to the observed eliminated is \textbf{\FitDiagMeanLogPModel}, compared to judges-only \textbf{\FitDiagMeanLogPJudgesOnly} and random \textbf{\FitDiagMeanLogPRandom} (higher is better). The true eliminated is often among the most likely: rank 1 in \textbf{\FitDiagTrueRankLEOnePct\%} of weeks, in the bottom two in \textbf{\FitDiagTrueRankLETwoPct\%}, and in the bottom three in \textbf{\FitDiagTrueRankLEThreePct\%}. Denominator: \FitDiagNWeeks\ elimination weeks. See \texttt{fit\_diagnostics.csv} and \texttt{fit\_diagnostics\_summary.tex} \cite{comap2026}.
  \item \textbf{Uncertainty.} We quantify uncertainty via (i) season-bootstrap intervals for $f_{i,t}$ and (ii) a margin-to-flip ``robustness radius'' measuring how much $f$ must change to alter the eliminated contestant. Certainty varies strongly by week; see \texttt{proposed\_system\_robustness.csv}.
  \item \textbf{Rule comparison.} From \texttt{season\_rule\_comparison.csv}: percent vs.\ rank disagreement rates and the fan-influence index (fraction of weeks a poor-judge/high-fan contestant survives; Section~\ref{sec:definition_audit}). Sensitivity sweep (flip = vs.\ previous grid point) uses \texttt{sensitivity\_flip\_summary.csv}.
  \item \textbf{Controversy cases.} Seasons 2, 4, 11, 27: judge--fan disagreement is visible in inferred shares vs.\ judge scores; outcomes differ under rank vs.\ percent and under judges-save (see controversy tables and counterfactual CSVs).
  \item \textbf{Drivers.} From \texttt{pro\_dancer\_effects\_top10.csv} and \texttt{judges\_fans\_comparison.csv}: pro dancer and celebrity characteristics affect judges and fans differently (e.g., pros with largest fan-minus-judge fixed effects).
  \item \textbf{Recommendation.} We recommend a \textbf{weighted percent rule with saturation} and an \textbf{optional judges-save trigger} in close weeks. The saturation softcaps \emph{fan} share to prevent extreme fan-bloc dominance (combined score $c = w\cdot j + (1-w)\cdot\text{softcap}(f)$). Evaluation reports (a) how often outcomes would change under the proposed rule, (b) controversy-mismatch reduction, and (c) robustness-radius improvements relative to the historical rule.
\end{itemize}
\newpage

% ============================================================
% Table of Contents
% ============================================================
\tableofcontents
\newpage

% ============================================================
% 2) Main Report
% ============================================================
\section{Problem Context and Data}
\subsection{Competition and Rule Regimes}
The show combines judges' scores and fan votes to eliminate the couple with the lowest combined score each week; fans vote to keep a couple (not to eliminate one), and judges score each performance \cite{comap2026}. In the first two U.S. seasons, the combination used \emph{ranks}; beginning in season 3, producers switched to a \emph{percent} method after season 2 concerns, and in response to a season 27 controversy, a judges-save modification (choose which of the bottom two to eliminate) was introduced around season 28 together with a return to the rank-based method \cite{comap2026}. The exact season is not known, but assuming season 28 is reasonable.

\subsection{Assumptions and Notation}
We assume: (i) season 28 is the rule-change point (judges-save and return to rank); (ii) index scaling (e.g., $V_i = f_i \times 10^7$) is a normalization for interpretability only, not true vote counts \cite{comap2026}; (iii) we work week-by-week over the \emph{active set} $A_t$ defined operationally from the data: contestants with a strictly positive total judges score in week $t$ (Section~1.3). Notation: $A_t$ = active contestants in week $t$; $J_{i,t}$ = total judges score; $f_{i,t}$ = fan vote share; $j_{i,t}$, $c_{i,t}$, $R_{i,t}$ as in the voting schemes below.

\subsection{Dataset and Preprocessing}
\paragraph{Raw CSV structure.}
The provided CSV is \emph{one row per couple} (celebrity and ballroom partner) with identifying fields (e.g., \texttt{season}, \texttt{celebrity\_name}, \texttt{ballroom\_partner}), outcome fields (\texttt{results}, \texttt{placement}), and a wide block of judges' scores with columns of the form
\[
\texttt{week\{$k$\}\_judge\{$j$\}\_score},
\]
which record judge $j$'s score for that couple in week $k$ (when present). Season lengths vary, so the set of \texttt{week\{$k$\}\_...} columns is irregular across seasons.

\paragraph{Long and contestant--week panels.}
We reshape the wide judge-score block to a long panel with one row per (couple, season, week, judge) by parsing $k$ and $j$ from the column names, then aggregate back to a contestant--week panel. In aggregation we compute:
\begin{itemize}[nosep]
  \item \texttt{score\_week\_total}: sum of non-missing judge scores that week;
  \item \texttt{num\_judges}: number of non-missing judge scores contributing to the total.
\end{itemize}
All subsequent rule simulation and model fitting operate on the contestant--week panel.

\paragraph{Definition of the active set.}
A couple is \textbf{active} in week $t$ iff \texttt{score\_week\_total}$>0$ that week. This operational definition matches the data: COMAP notes that 0 scores are recorded after elimination, and the score block has missing/N/A for some judges or weeks. Using \texttt{score\_week\_total}$>0$ thus filters the participating field without relying on text parsing. (We still parse \texttt{results} for \texttt{elimination\_week}; \texttt{active} is defined from scores only.) All combined-score and elimination logic uses $A_t$.

\paragraph{Elimination week parsing.}
For each couple we assign an \textbf{elimination\_week} (the week they leave the competition) using a rule that prioritizes explicit labels but falls back to score structure:
\begin{itemize}[nosep]
  \item \textbf{From \texttt{results}:} if \texttt{results} contains the pattern ``Eliminated Week $k$'' (case-insensitive), we set \texttt{elimination\_week}$=k$.
  \item \textbf{From scores:} otherwise we set \texttt{elimination\_week} to the first week where \texttt{score\_week\_total} is 0 and the previous week had positive total.
\end{itemize}
A (season, week) is treated as having an \textbf{observed elimination} if at least one \emph{active} couple has \texttt{elimination\_week} equal to that week.

\paragraph{Edge cases and structural quirks.}
The preprocessing step explicitly handles the dataset's main irregularities:
\begin{itemize}[nosep]
  \item \textbf{Missing and N/A judges:} blanks and the literal string ``N/A'' are treated as missing scores (NaN) in the long panel; zeros remain zeros. A contestant--week with all judges missing has \texttt{score\_week\_total}$=0$ and is therefore inactive.
  \item \textbf{Withdrawals:} if \texttt{results} contains ``Withdrew,'' we treat the couple as leaving after their last week with \texttt{score\_week\_total}$>0$; they are inactive thereafter.
  \item \textbf{No-elimination weeks:} some weeks have an active set but no observed elimination (specials/finals). Included in forward simulation and rule comparison; excluded from consistency evaluation.
  \item \textbf{Double eliminations:} if multiple couples share the same \texttt{elimination\_week}, we record the week as an elimination week but (for one-to-one diagnostics) take the first such couple as the ``observed eliminated'' for consistency calculations.
\end{itemize}

\paragraph{Why 265 vs.\ 335 weeks appear.}
Two denominators recur in the report:
\begin{itemize}[nosep]
  \item \textbf{335} = all (season, week) forward-event weeks with at least two active couples (so rules and forward fan-share inference apply).
  \item \textbf{265} = the subset of those weeks with an \emph{observed eliminated couple} (used for inverse-fit consistency metrics).
\end{itemize}
Thus 335 supports rule comparisons and robustness analyses; 265 supports fit diagnostics.

\paragraph{Reproducibility (pipeline).}
\begin{itemize}[nosep]
  \item \textbf{One command:} \texttt{python main.py} regenerates all tables in \texttt{reports/tables/} (e.g., \texttt{fit\_diagnostics.csv}, \texttt{season\_rule\_comparison.csv}, \texttt{judges\_save\_alpha.json}).
  \item \textbf{Consistency checks:} \texttt{run\_gonogo\_checks.py} validates rule regime mapping and elimination parsing (\texttt{reports/gonogo\_report.md}).
  \item \textbf{No hidden numbers:} all numeric claims in this report either come from LaTeX macros generated by the pipeline or from named artifacts in \texttt{reports/tables/}.
\end{itemize}

\subsection{Definition audit (paper--code alignment)}
\label{sec:definition_audit}
To avoid definitional mismatches between prose and implementation, we state precisely:

\textbf{Fan influence index (FII).} In this report and in \texttt{season\_rule\_comparison.csv}, FII is the \emph{fraction of elimination weeks in which at least one contestant with poor judge score} (bottom half of $J$ in that week) \emph{but high fan share} (top half of $f$) \emph{survives} (is not eliminated) under that rule---i.e., ``how often does a poor-judge/high-fan contestant survive?'' This matches the code in \texttt{src/analysis/counterfactual\_engine.py}. We do \emph{not} use the alternative definition ``fraction of weeks where the eliminated couple would change if fan share were neutralized.''

\textbf{Sensitivity flips (\texttt{sensitivity\_flip\_summary.csv}).} A \emph{flip} is defined \emph{only} as: the eliminated contestant at this grid point $w_J$ \emph{differs from the eliminated contestant at the previous grid point} (not vs.\ historical observed elimination). The sweep uses $c_i = w_J\, j_i + (1-w_J)\, f_i$ over a grid of $w_J$; for each (season, week) we record whether the eliminated index changes when moving to the next $w_J$. So \texttt{n\_flips} at $w_J=1.0$ means 26 weeks flip when moving from $w_J=0.98$ to $1.0$; at $w_J=0$ there is no previous grid point, so flips are not defined (we record 0). This definition is used consistently wherever we refer to sensitivity flips or \texttt{sensitivity\_flip\_summary.csv}.

\subsection{Output map}
All tables produced by \texttt{python main.py} appear in \texttt{reports/tables/}\ldots

\section{Voting Schemes: Rank, Percent, and Judges-Save}
Let $A_t$ be active contestants in week $t$ and $J_{i,t}$ the total judges score.
\subsection{Percent scheme}
Define judges percent $j_{i,t}=J_{i,t}/\sum_{k\in A_t}J_{k,t}$ and fan percent $f_{i,t}=V_{i,t}/\sum_{k\in A_t}V_{k,t}$. Combined score $c_{i,t}=j_{i,t}+f_{i,t}$; eliminated is $\argmin_i c_{i,t}$.
\subsection{Rank scheme}
Let $r^J_{i,t}$ be rank of $J_{i,t}$ (best=1) and $r^F_{i,t}$ rank of $V_{i,t}$; combined $R_{i,t}=r^J_{i,t}+r^F_{i,t}$; eliminated is $\argmax_i R_{i,t}$.
\subsection{Judges-save modification}
Among the bottom two by combined criterion, judges select which couple to eliminate; we model this probabilistically with
\[
\Pr(\text{eliminate }i\mid\{i,k\})=\sigma\bigl(\alpha(J_{k,t}-J_{i,t})\bigr),
\]
and fit $\alpha$ from observed outcomes in the judges-save era.\par\noindent Fitted $\alpha=\AlphaJudgesSave$ (see \texttt{judges\_save\_alpha.json}).

\subsection{Rule simulators}
Our code implements these rules as deterministic simulators for counterfactuals and consistency checks.

\paragraph{Percent vs.\ rank tie-breaking.}
\begin{itemize}[nosep]
  \item \textbf{Percent:} Combined score $c_i = j_i + f_i$; eliminated = $\argmin_i c_i$. Ties (multiple contestants with the same minimum $c$) are broken by \emph{index order}: we take the first index achieving the minimum (e.g., \texttt{np.argmin}, which returns the first occurrence). So the contestant who appears first in the active list among those tied for worst is eliminated.
  \item \textbf{Rank:} Judge ranks $r^J_i$ and fan ranks $r^F_i$ use \emph{average} ranks when raw scores tie (e.g., \texttt{scipy.stats.rankdata} with \texttt{method='average'}). Combined $R_i = r^J_i + r^F_i$; eliminated = $\argmax_i R_i$ (worst rank-sum). Tie-breaking for ``who is eliminated'' is again by index order: the first index achieving the maximum is eliminated.
\end{itemize}
So in both schemes, a tie on the combined metric is resolved deterministically by choosing the contestant with the smaller index in the active set.

\paragraph{Judges-save bottom-two logic.}
In seasons that use judges save (we assume from season 28 onward), the simulator does not eliminate by combined score alone. Steps:
\begin{enumerate}[nosep]
  \item \textbf{Bottom two:} Identify the two contestants with the \emph{worst} combined score. For the rank rule, $R_i$ is the rank-sum (higher = worse), so the bottom two are the two with \emph{largest} $R$. Ties (e.g., who is ``second-worst'') are broken by index order: we sort by combined score and take the last two indices (stable argsort), so the two worst are uniquely chosen.
  \item \textbf{Judges' choice:} Among these two, the contestant with the \emph{lower} judges' total $J$ is eliminated; the one with the higher $J$ is ``saved.'' Thus judges save the higher-scoring of the bottom two. If $J$ is tied for the two, we break the tie by index (the second of the two indices is eliminated in our implementation).
\end{enumerate}
The probabilistic model $\Pr(\text{eliminate }i\mid\{i,k\})=\sigma(\alpha(J_k-J_i))$ is used only for \emph{fitting} $\alpha$ from observed outcomes; the rule simulator used for counterfactuals and consistency is deterministic (lower $J$ among bottom two $\to$ eliminated).

\section{Model for Latent Fan Vote Shares}
\subsection{Identifiability}
Under the percent rule, only \emph{shares} $f_{i,t}=V_{i,t}/\sum_{k\in A_t}V_{k,t}$ are identified: vote totals can be scaled arbitrarily without changing combined scores $c_{i,t}=j_{i,t}+f_{i,t}$. Under the rank rule, only the \emph{ordering} of vote totals is identified. We therefore report fan vote \emph{shares} as primary outputs and provide index-scaled totals (e.g., $V_i = f_i \times 10^7$) only for interpretability, with a clear label that they are not true vote counts. This non-identifiability is also illustrated in the problem appendix, which notes that many hypothetical fan vote totals can reproduce the same eliminations and presents an example using an arbitrary total of 10 million votes \cite{comap2026}.

\paragraph{Model status and reproducibility.}
All fitted parameters ($\beta$, $\tau$, and contestant baselines $\theta$) are saved to \texttt{reports/tables/fitted\_params.json}. The pipeline halts if the optimizer does not converge, so every reported metric corresponds to a successful fit and can be reproduced by re-running \texttt{python main.py}.

\subsection{Latent preference model}
Let $A_t$ denote the set of active contestants in week $t$. We model fan share as a multinomial logit (softmax) over a latent utility $u_{i,t}$:
\[
f_{i,t} = \frac{\exp(u_{i,t})}{\sum_{k\in A_t}\exp(u_{k,t})},
\]
with
\[
u_{i,t} = \theta_i + \beta_0 + \beta_J\,\tilde J_{i,t} + \beta_P\,\text{momentum}_{i,t} + \beta_U\,\text{underdog}_{i,t} + \beta_X^\top X_i.
\]
Here $\theta_i$ is a \emph{contestant-specific baseline} (unobserved popularity or fanbase) for contestant $i$, one per (season, celebrity, partner); it captures variation in baseline appeal that age and industry cannot. The coefficients $\beta = (\beta_0,\beta_J,\beta_P,\beta_U,\beta_X)$ are shared across seasons and weeks. Terms are defined as follows. \textbf{Normalized judge score} $\tilde J_{i,t}$: within each $(season, week)$, we set $\tilde J_{i,t} = J_{i,t} / \max_{k\in A_t} J_{k,t}$ (and 0 if the max is 0), so that the best-scoring contestant has $\tilde J=1$. This captures the extent to which judges favor contestant $i$ relative to the field. \textbf{Momentum} (denoted $p_{\text{prev}}$ in code): rank by judges' score among \emph{active} contestants in the previous week (1 = best); for week 1 we set it to 0. Higher rank last week may attract more fan attention (momentum) or, if negative, an ``underdog'' effect. \textbf{Underdog}: we set $\text{underdog}_{i,t}=1$ if $J_{i,t}$ is at or below the median judges score among active contestants in that week, else 0. This allows a ``rage vote'' or sympathy effect for lower-scoring contestants. \textbf{Covariates} $X_i$: we include celebrity age (during the season) and an industry dummy (e.g., 1 if Actor/Actress). The season-appropriate rule (percent or rank) is applied when mapping $(J,f)$ to combined scores and thus to elimination.

\subsection{Likelihood and fitting}
We fit $\beta$ and a temperature $\tau>0$ by maximum likelihood. The likelihood has two parts.

\textbf{Elimination events.} For each week with an elimination, let $c_{i,t}$ (percent) or $R_{i,t}$ (rank) be the combined score under the rule for that season. We model the probability that contestant $i$ is eliminated as a softmax over the active set. Under percent: lower combined score $c$ implies higher elimination probability; we set
\[
\Pr(\text{eliminate }i) \propto \exp(-\tau\, c_{i,t}).
\]
Under rank: higher rank-sum $R$ (worse) implies higher elimination probability; we set
\[
\Pr(\text{eliminate }i) \propto \exp(\tau\, R_{i,t}).
\]
Thus $\tau$ controls determinism: large $\tau$ makes the lowest $c$ (or highest $R$) almost surely eliminated; small $\tau$ flattens the distribution.

\textbf{Finals ordering.} For each season's finals week, we observe the placement order (1st, 2nd, 3rd). We model this with a Plackett--Luce likelihood: the probability of the observed ordering given strengths $s_i$ (we use combined score for percent, or negative rank-sum for rank so that better rank-sum gives higher strength) is the product over positions of the probability of choosing that contestant from the remaining set, with choice probabilities proportional to $\exp(s_i)$.

The total log-likelihood is the sum of log-probabilities over all elimination events plus the sum over all finals events. We minimize the \emph{negative} log-likelihood plus an L2 (ridge) penalty on the contestant baselines to avoid overfitting:
\[
\min_{\beta,\theta,\tau} \; -\log L(\beta,\theta,\tau) + \lambda \sum_i \theta_i^2.
\]
We use L-BFGS-B over $(\beta,\theta,\tau)$, with $\tau$ bounded below by a small positive constant and $\lambda$ a fixed regularization strength (e.g., 0.5). Covariates and contestant indices are built from the contestant--week data; see \texttt{src/models/vote\_latent.py} and \texttt{src/fit/fit\_elimination.py}. Fit diagnostics (elimination match rates, finals likelihood) are reported in Section~\ref{sec:fitquality}.

\section{Uncertainty Quantification}
We measure uncertainty in two complementary ways.

\textbf{(1) Bootstrap intervals for fan shares.} We resample \emph{seasons} with replacement (e.g., 5 bootstrap replicates). For each replicate we refit $(\beta,\tau)$ on the resampled contestant--week data, then run the forward pass on the \emph{full} dataset with the fitted $\beta$ to obtain fan shares $f_{i,t}$ for every $(season, week, contestant)$. Across replicates we compute the mean, 5th percentile, and 95th percentile of $f_{i,t}$ at each cell. This yields interval estimates for $f_{i,t}$ that reflect uncertainty due to season-to-season variation in the elimination and finals data. Optionally, we can bootstrap by resampling \emph{weeks} within each season instead of seasons; the implementation supports both (see \texttt{src/fit/uncertainty.py}).

\begin{table}[H]
\centering
\caption{Bootstrap interval widths for inferred fan shares $f_{i,t}$ (computed from \texttt{fan\_shares\_bootstrap\_intervals.csv}).}
\label{tab:bootstrap_widths}
\begin{tabular}{lr}
\toprule
Statistic & Value\\
\midrule
Number of contestant-week cells & \BootstrapNCells\\
Median width ($f_{0.95}-f_{0.05}$) & \BootstrapWidthMedian\\
IQR width ($Q_{0.75}-Q_{0.25}$) & \BootstrapWidthQOne\ -- \ \BootstrapWidthQThree\\
95th percentile width & \BootstrapWidthP95\\
\bottomrule
\end{tabular}
\end{table}

\textbf{(2) Margin-to-flip robustness radius.} For each elimination week we ask: how much must fan shares $f_t$ change so that a \emph{different} contestant would be eliminated under the same rule? We parameterize perturbations in log-share space: $f' = \mathrm{softmax}(\log f + z)$ with $z$ unconstrained, so that $f'$ remains on the simplex. We find the minimum L2 norm of $z$ such that the eliminated contestant under the week's rule (percent: $\argmin_i (j_i + f'_i)$; rank: $\argmax_i (r^J_i + r^F_i)$ with ranks from $f'$) is not the currently eliminated contestant. The optimization is a constrained nonlinear problem (minimize $\|z\|^2$ subject to the flip constraint); we use SLSQP. For interpretability we also report the L2 norm of $(f'-f)$ in share space as the ``robustness radius.'' A \emph{small} radius means the outcome is sensitive to small changes in fan shares (uncertain week); a \emph{large} or infinite radius means the eliminated contestant would not change under plausible perturbations (certain week). See \texttt{src/fit/margin\_to\_flip.py}.

\textbf{Examples.} Tight week: Season~10 Week~1, robustness radius $0$ (outcome sensitive to small changes in fan shares). Blowout week: Season~12 Week~3, robustness radius $\infty$ (no feasible perturbation changes the eliminated contestant; elimination is robust). Certainty varies by week and contestant because of the geometry of combined scores near the elimination boundary: when the bottom two are close, a small shift in $f$ can flip who is eliminated; when one contestant is clearly last, the radius is large or infinite.

Table~\ref{tab:robustness_summary} shows the distribution of margin-to-flip robustness radii under the proposed scoring rule (weighted saturation). Knife-edge weeks (radius $0$) and blowouts (radius $\infty$) are reported from \texttt{proposed\_system\_robustness.csv} via \texttt{robustness\_summary.tex}.

\begin{table}[H]
\centering
\caption{Distribution of margin-to-flip robustness radii under the proposed scoring rule (weighted saturation). Radii are computed per (season, week) elimination event; larger means more robust.}
\label{tab:robustness_summary}
\begin{tabular}{lr}
\toprule
Quantity & Value\\
\midrule
Number of elimination weeks & \RobustNWeeks\\
Zero-radius (knife-edge) weeks & \RobustZeroCount\ (\RobustZeroPct\%)\\
Infinite-radius (blowout) weeks & \RobustInfCount\ (\RobustInfPct\%)\\
\midrule
Finite-radius median & \RobustMedian\\
Finite-radius IQR & \RobustQOne\ -- \ \RobustQThree\\
Finite-radius max & \RobustMax\\
\bottomrule
\end{tabular}
\end{table}

\section{Rank vs Percent Across Seasons}

\subsection{Across-season disagreement rates and fan influence}

Table~\ref{tab:rule_compare_stats} summarizes how often different elimination rules disagree about who goes home, and reports the fan influence index (FII). All statistics in this section are computed from \texttt{season\_rule\_comparison.csv} (per-season) and the definition in Section~\ref{sec:definition_audit}. We interpret higher disagreement as greater sensitivity to the combination rule, and higher FII as stronger fan impact (poor-judge/high-fan contestants survive more often). Table~\ref{tab:top_disagree_seasons} lists the highest-disagreement seasons from the same artifact.

\begin{table}[H]
\centering
\caption{Across-season disagreement between elimination rules and fan influence (computed from \texttt{season\_rule\_comparison.csv}). Each season contributes its fraction of elimination weeks where the eliminated couple differs under the two rules. Fan influence index (FII) is the fraction of weeks a poor-judge/high-fan contestant survives (Section~\ref{sec:definition_audit}).}
\label{tab:rule_compare_stats}
\begin{tabular}{lcccc}
\toprule
Statistic & pct vs rank & pct vs save & rank vs save & fan infl.\ (pct / rank)\\
\midrule
\IfFileExists{tables/season_rule_comparison_summary.tex}{\input{tables/season_rule_comparison_summary.tex}}{
Min & 0.0\% & 0.0\% & 0.0\% & 0.000 / 0.000\\
Median & 0.0\% & 0.0\% & 0.0\% & 0.000 / 0.000\\
Mean & 0.0\% & 0.0\% & 0.0\% & 0.000 / 0.000\\
Max & 0.0\% & 0.0\% & 0.0\% & 0.000 / 0.000\\
}
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Seasons with the largest disagreement between percent and rank elimination outcomes (from \texttt{season\_rule\_comparison.csv}).}
\label{tab:top_disagree_seasons}
\begin{tabular}{rrrr}
\toprule
Season & Elim.\ weeks & pct vs rank diff & fan infl.\ (pct / rank)\\
\midrule
\IfFileExists{tables/season_rule_comparison_top.tex}{\input{tables/season_rule_comparison_top.tex}}{
0 & 0 & 0.0\% & 0.000 / 0.000\\
0 & 0 & 0.0\% & 0.000 / 0.000\\
0 & 0 & 0.0\% & 0.000 / 0.000\\
0 & 0 & 0.0\% & 0.000 / 0.000\\
}
\bottomrule
\end{tabular}
\end{table}

From \texttt{sensitivity\_flip\_summary.csv} (flip = vs previous grid point only): the fraction of weeks where elimination flips varies across the judge-weight grid, capturing sensitivity to small rule changes.

\section{Controversy Case Studies}
We examine the prompt's highlighted controversy examples \cite{comap2026}: Season 2 (Jerry Rice), Season 4 (Billy Ray Cyrus), Season 11 (Bristol Palin), and Season 27 (Bobby Bones). For each season we include two compact, artifact-based tables:\par
\textbf{(i) Fan vs.\ judges disagreement:} \texttt{controversy\_seasonX\_fan\_shares\_vs\_judges.csv} (contestant-week rows with inferred fan shares alongside judge totals).\par
\textbf{(ii) Counterfactual eliminations:} \texttt{controversy\_seasonX\_counterfactual\_elimination.csv} (per-week eliminated under percent, rank, and judges-save).\par
We highlight weeks where the rule choice changes the eliminated contestant and connect those to the reported controversy narratives.

\section{Drivers of Performance: Pro Dancer and Celebrity Characteristics}
\label{sec:drivers}

We estimate two parallel linear models: one for judges outcomes and one for inferred fan outcomes, using the same core covariates.
Table~\ref{tab:judges_fans_covars} compares the common covariates side-by-side.

Two effects are strong and consistent across judges and fans:
\textbf{week} has a negative coefficient in both models, and \textbf{age} is strongly negative in both models.
Industry and region behave differently: \textbf{industry\_dummy} is positive and highly significant for judges but near zero and not significant for fans,
suggesting judges respond to industry-related factors more than the voting public does.
The \textbf{region\_us} indicator is modestly negative and significant for judges but not significant for fans.

(Separately, pro-dancer fixed effects provide the largest heterogeneity in the cross-sectional fit; we summarize those in \texttt{pro\_dancer\_effects\_top10.csv} and \texttt{pro\_dancer\_effects\_bottom10.csv}. Pros with largest positive ``fan minus judge'' effect: Henry Byalikov, Andrea Hale; judges-favoring: Koko Iwasaki, Ashly DelGrosso.)

\begin{table}[H]
\centering
\caption{Common-covariate comparison for judges vs.\ fans regressions. OLS coefficients and $p$-values; stars correspond to conventional significance levels.}
\label{tab:judges_fans_covars}
\begin{tabular}{lrrrr}
\toprule
Covariate & Judges coef.\ & $p$-value & Fans coef.\ & $p$-value\\
\midrule
week & $-0.042$ *** & $1.21\times10^{-11}$ & $-0.046$ *** & $2.22\times10^{-15}$\\
age & $-0.033$ *** & $3.38\times10^{-91}$ & $-0.048$ *** & $2.83\times10^{-172}$\\
industry\_dummy & $0.183$ *** & $3.05\times10^{-6}$ & $0.010$ & $0.79$\\
region\_us & $-0.134$ ** & $1.29\times10^{-2}$ & $-0.081$ & $0.11$\\
\bottomrule
\end{tabular}
\end{table}

\section{Proposed ``Better'' System and Evaluation}
We propose a \textbf{weighted percent with saturation} and an optional trigger-based judges-save. The combined score is $c_i = w\cdot j_i + (1-w)\cdot\text{softcap}(f_i)$; the softcap is applied to \emph{fan} share to prevent extreme fan-bloc dominance (not judge dominance). Optionally: trigger judges-save when the bottom-two margin is below a threshold. Fairness axioms: monotonicity (better combined score $\Rightarrow$ not eliminated), fan relevance bounds, robustness (margin-to-flip), transparency.
\par\textbf{Evaluation outputs (artifact-aligned).}
\begin{itemize}[nosep]
  \item \textbf{Scope of change:} \texttt{proposed\_system\_eval.csv} reports whether the proposed rule eliminates the same contestant as observed for each (season, week).
  \item \textbf{Controversy mismatch reduction:} the evaluation reports how often a judges-favored but fan-disfavored contestant is eliminated under observed vs.\ proposed outcomes.
  \item \textbf{Predictability:} robustness radii for the proposed rule are in \texttt{proposed\_system\_robustness.csv}; the current system’s radii are in \texttt{current\_system\_robustness.csv} for direct comparison.
  \item \textbf{Parameter sensitivity:} \texttt{proposed\_system\_sensitivity.csv} reports how the fraction of changed eliminations varies across $(w,\alpha)$.
\end{itemize}

\section{Fit Quality and Validation}
\label{sec:fitquality}
\begin{itemize}[leftmargin=*]
  \item \textbf{Sample sizes:} inverse-fit diagnostics use \FitDiagNWeeks\ elimination weeks, split into \FitDiagNPercent\ percent-rule weeks and \FitDiagNRank\ rank-rule weeks (full universe 335 weeks) \cite{comap2026}.
  \item \textbf{Proper scoring (log probability of observed elimination):} overall mean log probability is \FitDiagMeanLogPModel, compared to judges-only \FitDiagMeanLogPJudgesOnly and random \FitDiagMeanLogPRandom. By regime, the model’s mean log probability is \FitDiagMeanLogPPercentModel\ (percent) and \FitDiagMeanLogPRankModel\ (rank); see \texttt{fit\_diagnostics.csv}.
  \item \textbf{Point prediction (MAP match):} overall MAP match is \FitDiagMAPMatchModelPct\% (judges-only \FitDiagMAPMatchJudgesOnlyPct\%). By regime, the model matches the observed eliminated in \FitDiagMAPMatchPercentModelPct\% of percent weeks and \FitDiagMAPMatchRankModelPct\% of rank weeks; judges-only regime rates are \FitDiagMAPMatchPercentJudgesOnlyPct\% and \FitDiagMAPMatchRankJudgesOnlyPct\%.
  \item \textbf{Coverage / ranking of the observed eliminated:} mean rank is \FitDiagTrueRankMean; the observed eliminated is the most likely in \FitDiagTrueRankLEOnePct\% of weeks, in the bottom two in \FitDiagTrueRankLETwoPct\%, and in the bottom three in \FitDiagTrueRankLEThreePct\%.
  \item \textbf{Baselines (definition):} random elimination among active (Pr$=1/n$) and judges-only (percent: $c=j$; rank: $R=r^J$) are reported per-week in \texttt{fit\_diagnostics.csv} for direct comparison.
  \item \textbf{Holdout check:} we also evaluate on held-out seasons using \texttt{fit\_diagnostics\_holdout.csv} to confirm performance transfers beyond the fit sample.
  \item \textbf{Finals likelihood:} Plackett--Luce term is non-degenerate; ordering likelihood contributes to identification.
\end{itemize}

\section{Limitations and Extensions}
Identifiability: only shares (percent) or order (rank) are identified. Regime uncertainty: season~28 judges-save start is assumed. Unobserved confounders: marketing, social media, contestant visibility. Extensions: incorporate viewership or social sentiment if data become available.

% ============================================================
% Appendix (optional): Mean-field / network interpretation
% ============================================================
\appendix
\section{Mechanistic Interpretation via Biased Mean-Field Voter Dynamics (Optional)}
\label{sec:meanfield}

The mean-field/memory/network module (in \texttt{src/models/meanfield.py}) is used for \emph{interpretation and robustness simulation}, not as the primary inference engine. It provides a mechanistic story for how judge scores and social influence could generate vote shares over time; we treat the $\beta$-fitted latent vote model in \texttt{main.py} as the primary estimator. Below we summarize the structure so that results from this module can be interpreted consistently with the main report.

\subsection{Role relative to the primary model}
The primary model (Section~3) fits a single set of coefficients $\beta$ and temperature $\tau$ by maximum likelihood so that implied eliminations match observed outcomes. That model is \emph{static} in the sense that fan share in week $t$ depends on covariates and judge score in week $t$ (and optionally momentum/underdog from the same or previous week), but there is no explicit dynamical law linking $p_t$ to $p_{t-1}$. The mean-field module adds a \emph{discrete-time dynamical} layer: fan shares evolve week-to-week via a recurrence $p_{t+1} = \Phi(p_t, S_t, \ldots)$. This can be used to (i) interpret how judge signals and past popularity might combine into a plausible evolution of votes, and (ii) run robustness or scenario simulations (e.g., different memory kernels or network coupling) without re-fitting the primary $\beta$.

\subsection{Single-population mean-field update (F2)}
The core recurrence is
\[
p_{t+1} = (1 - \kappa)\, p_t + \kappa\, \mathrm{softmax}(u_t), \qquad u_t = \eta\, S_t + \gamma\, \log(p_t + \varepsilon) + \text{(optional terms)}.
\]
Here $p_t$ is the vector of fan shares (simplex), $S_t$ is the vector of judge signals (e.g., normalized scores) in week $t$, and $\kappa \in (0,1]$ is a mixing speed: the new share is a convex combination of the previous share and a softmax of the utility $u_t$. The term $\gamma\, \log(p_t)$ captures \emph{incumbency} or social reinforcement (higher current share $\Rightarrow$ higher utility, all else equal). The term $\eta\, S_t$ is the judge-driven component. Optional terms include covariates $\theta^\top X$, an underdog term $\beta_U\,\text{underdog}_t$, and memory terms described below.

\subsection{Switching-rate microfoundation (F1)}
One can motivate the update by a \emph{switching-rate} story: voters are drawn toward a target distribution $q_t$ that mixes current popularity and a judge-driven distribution. For example, $q_t = \rho\, p_t + (1-\rho)\,\mathrm{softmax}(\eta\, S_t)$, normalized. Then $p_{t+1} = (1-\kappa)\, p_t + \kappa\, q_t$ corresponds to a mean-field law where a fraction $\kappa$ of the population ``switches'' toward $q_t$ each period. The implementation supports this via the same recurrence with $u_t$ constructed so that $\mathrm{softmax}(u_t)$ matches the desired target (e.g., with $\gamma$ and $\eta$ playing the roles of $\rho$ and judge weight).

\subsection{Underdog / rage-vote (F3)}
The underdog score is a scalar per contestant that is high when the contestant has \emph{low} judge score but \emph{high} current popularity (or vice versa, depending on parameterization). Formally, $\text{underdog}_i = \sigma(a(p_i - \tau_p))\,\sigma(b(\tau_S - S_i))$ in smooth mode, or an indicator $1[S_i \le \tau_S]\, 1[p_i \ge \tau_p]$ in indicator mode, with thresholds $\tau_S$, $\tau_p$ (e.g., medians). This is added to $u_t$ as $\beta_U\,\text{underdog}_t$, so that ``underdog'' contestants get a utility boost and can sustain higher share despite lower judge scores---a simple model of sympathy or rage voting.

\subsection{Memory: kernel-weighted history and Markovian state}
Two types of memory are supported. (1) \textbf{Kernel-weighted history:} define $\bar{S}_t = \sum_{\tau=0}^t k(t-\tau)\, S_\tau$ and $\bar{p}_t$ similarly, where $k(\Delta t)$ is a kernel (e.g., exponential $k(\Delta t) = \lambda^{\Delta t}$, rectangular window, or power-law). Then $u_t$ can include $\eta_S\, \bar{S}_t$ and $\gamma_{\text{hist}}\, \log(\bar{p}_t + \varepsilon)$, so that past judge signals and past popularity influence current utility. (2) \textbf{Markovian fading state:} a single state vector $m_t$ is updated by $m_{t+1} = (1-\lambda)\, m_t + \lambda\, S_t$, and $u_t$ includes $\eta_m\, m_t$. This gives a one-dimensional fading memory of judge signals. Both channels are optional; when omitted, the model reduces to the static-like update with only $S_t$ and $\log(p_t)$.

\subsection{Networked extension: multiple communities and small-world coupling}
The code also supports $G$ ``communities'' (e.g., demographic or regional voter blocs), each with its own share vector $p_t^{(g)}$ on the simplex. Communities are coupled via a row-stochastic influence matrix $W$ (e.g., from a Watts--Strogatz small-world graph: ring lattice with $K$ neighbors, then each edge rewired with probability $\beta$; then $W$ is adjacency plus self-weight $\omega_{\mathrm{self}}$, row-normalized). The utility in community $g$ is
\[
u_t^{(g)} = \eta\, S_t + \gamma\, \log(p_t^{(g)} + \varepsilon) + \delta\, (W \log(p_t + \varepsilon))_g + \text{(covariates, underdog)}.
\]
The term $(W \log p_t)_g$ is the weighted average of $\log p_t^{(h)}$ over neighbors $h$ of $g$, so that high popularity in neighboring communities boosts utility in $g$ (social influence across blocs). The aggregate share reported for elimination can be $\bar{p}_t = \sum_g w_g\, p_t^{(g)}$ for reporting weights $w_g$. Optional logit-normal noise ($\sigma_{\mathrm{shock}}$) can be added to $u$ before softmax for robustness checks.

\subsection{Parameters and use in the report}
Key parameters: $\kappa$ (mixing speed), $\eta$ (judge weight), $\gamma$ (incumbency/social weight), $\delta$ (cross-community weight), $\beta_U$ (underdog weight), and memory parameters (kernel type, decay/window, $\eta_S$, $\gamma_{\text{hist}}$, $\lambda$, $\eta_m$). These are \emph{not} fit by the same MLE as the primary model; they can be set for scenario or sensitivity analysis. In the main report we do not report point estimates from the mean-field module; we use it only to interpret how dynamics and memory could generate patterns consistent with the primary $\beta$-fit and to run robustness simulations (e.g., different kernels or coupling strength) when needed.

% ============================================================
% 3) Memo (1–2 pages)
% ============================================================
\newpage
\section*{Memo to DWTS Producers (1--2 pages)}
\textbf{To:} Producers of \DWTS \\
\textbf{From:} Team \#\_\_\_\_ \\
\textbf{Subject:} Recommended method for combining judges and fan votes \\
\vspace{0.5em}

\textbf{Executive recommendation.} We recommend adopting a \textbf{weighted percent rule with saturation} and an \textbf{optional judges-save trigger} in close weeks. The saturation softcaps \emph{fan} share to prevent extreme fan-bloc dominance (combined score $c = w\cdot j + (1-w)\cdot\text{softcap}(f)$), keeping fan votes meaningful without letting a single fan bloc dominate. The data show that percent and rank disagree on who goes home in only about 13\% of elimination weeks on average (range 0--36\% by season), and fan influence is already high (index 0.33--1.0, mean 0.92). The prompt's controversy examples---Season 2 (Jerry Rice), Season 4 (Billy Ray Cyrus), Season 11 (Bristol Palin), Season 27 (Bobby Bones)---show clear judge--fan disagreement; a judges-save can mitigate those cases \cite{comap2026}.

\textbf{Evidence.}
\begin{itemize}[leftmargin=*]
  \item \textbf{Rule comparison (season\_rule\_comparison.csv):} Across 34 seasons, percent vs.\ rank disagree on who is eliminated in \textbf{0--36.4\%} of elimination weeks by season (mean \textbf{13.4\%}). Fan-influence index is \textbf{0.33--1.0} (mean 0.92) for both rules---fans already have substantial weight.
  \item \textbf{Consistency (fitted latent model):} Fitted $\beta$ and $\tau$ are in \texttt{fitted\_params.json}. Using our \emph{fitted latent fan-share model}, the MAP eliminated matches the observed in \textbf{\FitDiagMAPMatchPercentModelPct\%} of percent-weeks ($\FitDiagNPercent$) and \textbf{\FitDiagMAPMatchRankModelPct\%} of rank-weeks ($\FitDiagNRank$), well above a random baseline of roughly $1/n$. Mean log probability assigned to the observed eliminated is \textbf{\FitDiagMeanLogPModel}; the true eliminated is in the model's bottom two in \textbf{\FitDiagTrueRankLETwoPct\%} of weeks. Baselines (random and judges-only) are in \texttt{fit\_diagnostics.csv}; in particular, the model's mean log probability improves over both judges-only (\FitDiagMeanLogPJudgesOnly) and random (\FitDiagMeanLogPRandom). Inverse-fit sample: \FitDiagNWeeks\ elimination weeks; full universe 335 weeks \cite{comap2026}. See \texttt{fit\_diagnostics.csv}, \texttt{fit\_diagnostics\_summary.tex}, \texttt{fitted\_params.json}, \texttt{reports/gonogo\_report.md}.
  \item \textbf{Uncertainty and robustness:} Certainty varies by week. Season~10 Week~1 has robustness radius 0 (tight); Season~12 Week~3 has radius infinite (blowout). Our proposed rule evaluation reports robustness-radius improvements and controversy-mismatch reduction relative to the historical rule; we report the \emph{scope} of change (how often the proposed rule would yield a different elimination) separately from consistency.
  \item \textbf{Controversy seasons:} In seasons 2, 4, 11, and 27 (Jerry Rice, Billy Ray Cyrus, Bristol Palin, Bobby Bones), inferred fan shares often disagree with judge rankings. A judges-save option lets producers avoid outcomes that would outrage fans when the bottom two are close.
  \item \textbf{Pro/celebrity effects (pro\_dancer\_effects\_top10.csv):} Pros who boost \emph{fans} relative to judges: \textbf{Andrea Hale} (2.03), \textbf{Henry Byalikov} (1.01). Judges-favoring pros appear in the bottom of the effects table. A stable rule keeps competition fair across pros.
\end{itemize}

\textbf{Proposed system.} Combine judge and fan contributions as $c = w\cdot j + (1-w)\cdot\text{softcap}(f)$; the softcap is on \emph{fan} share to prevent extreme fan-bloc dominance. Optionally: when the bottom two are within a small margin, trigger a judges-save (we fit $\alpha=\AlphaJudgesSave$ from season~28+ data). This preserves fan relevance, satisfies monotonicity and transparency, and reduces controversy in close weeks.

\textbf{Expected impact.} (i) Fan influence remains high (fan-influence index 0.33--1.0, mean 0.92). (ii) Fairness improves via monotonicity and bounded fan-bloc dominance. (iii) Robustness: margin-to-flip analysis identifies which weeks are close; judges-save can be used only in those weeks. (iv) Controversy frequency can decrease when judges-save is triggered in disputed bottom-two situations.

% ============================================================
% References
% ============================================================
\newpage
\begin{thebibliography}{9}
\bibitem{comap2026}
COMAP, \emph{2026 MCM Problem C: Data With The Stars}, 2026.
% Add any other documented sources if you used them.
\end{thebibliography}

% ============================================================
% AI Use Report (not counted in 25 pages)
% ============================================================
\newpage
\section*{AI Use Report}
AI was used for (1)~\textbf{coding}---implementation, debugging, refactoring, and pipeline automation---and (2)~\textbf{checking the final version}---reviewing the report and code for consistency, clarity, and alignment with COMAP requirements. An example prompt used for the latter was: ``You are an ICM/MCM judge; read Section~X and the corresponding code/table. Check that the report's claims match the implementation and that definitions (e.g., Fan Influence Index, robustness radius) are used consistently.'' All results were reproduced by running \texttt{python main.py} and \texttt{run\_gonogo\_checks.py}; fit diagnostics, tables, and LaTeX inputs are generated from the pipeline. No external data were used beyond the provided dataset and documented sources.

\end{document}
