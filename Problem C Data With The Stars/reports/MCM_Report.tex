% ============================================================
% MCM 2026 Problem C — Data With The Stars
% Full submission: Summary Sheet + TOC + Report + Memo + Refs + AI Use
% Artifacts: reports/tables/*.csv, reports/tables/judges_save_alpha.json
% ============================================================
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

% ---------- Convenience macros ----------
\newcommand{\DWTS}{\textit{Dancing with the Stars}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}

\title{MCM 2026 Problem C: Data With The Stars}
\author{Team \#\_\_\_\_}
\date{}

\begin{document}
\maketitle

% ============================================================
% 1) One-page Summary Sheet
% ============================================================
\section*{Summary Sheet}
\begin{itemize}[leftmargin=*]
  \item \textbf{Goal.} Infer weekly fan vote \emph{shares} (latent) from judges' scores and elimination outcomes; compare rank vs.\ percent vote-combination rules; quantify drivers of judges vs.\ fans; propose and evaluate a better system.
  \item \textbf{Key modeling idea.} Fan shares are generated by a latent preference model
  \[
    f_{i,t} = \frac{\exp(u_{i,t})}{\sum_{k\in A_t}\exp(u_{k,t})},\quad
    u_{i,t}=\beta_0+\beta_J\,\tilde J_{i,t}+\beta_P\,\text{momentum}_{i,t}+\beta_U\,\text{underdog}_{i,t}+\beta_X^\top X_i,
  \]
  where $\tilde J$ is a normalized judges score and $X_i$ are celebrity/pro covariates.
  \item \textbf{Estimation.} $\beta$ is fit by maximum likelihood so that the implied eliminations (and finals ordering) match observed results under the season-appropriate rule (rank vs.\ percent), with a ``temperature'' $\tau$ controlling determinism.
  \item \textbf{Consistency (fit to eliminations).} Fitted $\beta$ and $\tau$ are saved in \texttt{reports/tables/fitted\_params.json} for reproducibility. Using the fitted latent fan-share model, the implied (MAP) eliminated contestant matches the observed in \textbf{38.2\%} of percent-rule elimination weeks (76/199) and \textbf{30.3\%} of rank-rule weeks (20/66), well above a random baseline of $\approx 17\%$ when $\sim 6$ contestants remain. Under a proper scoring rule, the mean log probability assigned to the observed eliminated is $\approx -1.94$ (higher is better). In \textbf{37.7\%} of elimination weeks the observed eliminated was the model's most likely (rank 1); in \textbf{60.8\%} they were in the bottom two (rank $\le 2$), and in \textbf{74.3\%} in the bottom three (rank $\le 3$), so the model often places the true eliminated among the most likely candidates. The denominator is the \emph{inverse-fit evaluation sample} (265 elimination weeks; full rule-comparison universe 335 weeks). See \texttt{reports/tables/fit\_diagnostics.csv} and \texttt{reports/gonogo\_report.md} \cite{comap2026}.
  \item \textbf{Uncertainty.} We quantify uncertainty via (i) season-bootstrap intervals for $f_{i,t}$ and (ii) a margin-to-flip ``robustness radius'' measuring how much $f$ must change to alter the eliminated contestant. Certainty varies strongly by week: Season~10 Week~1 radius $0$ (tight) vs.\ Season~12 Week~3 radius $\infty$ (blowout).
  \item \textbf{Rule comparison.} From \texttt{season\_rule\_comparison.csv}: percent vs.\ rank disagree on who is eliminated in \textbf{0--36.4\%} of elimination weeks by season (mean \textbf{13.4\%}); fan-influence index (fraction of weeks a poor-judge/high-fan contestant survives; see Section~\ref{sec:definition_audit}) is \textbf{0.33--1.0} (mean 0.92) for both percent and rank. Sensitivity sweep (flip = vs previous grid point): at $w_J=1.0$, 26 of 335 weeks flip; at $w_J=0$, 0 flips.
  \item \textbf{Controversy cases.} Seasons 2, 4, 11, 27: judge--fan disagreement is visible in inferred shares vs.\ judge scores; outcomes differ under rank vs.\ percent and under judges-save (see controversy tables and counterfactual CSVs).
  \item \textbf{Drivers.} From \texttt{pro\_dancer\_effects\_top10.csv}: pro dancer and celebrity characteristics affect judges and fans differently. Pros who boost \emph{fans} relative to judges (largest fan-minus-judge fixed effects): \textbf{Andrea Hale} (2.03), \textbf{Henry Byalikov} (1.01), Elena Grinenko (1.01), Ezra Sosa (0.87), Tyne Stecklein (0.71); judges-favoring pros appear in the bottom of the effects table.
  \item \textbf{Recommendation.} We recommend a \textbf{weighted percent rule with saturation} and an \textbf{optional judges-save trigger} in close weeks. The saturation softcaps \emph{fan} share to prevent extreme fan-bloc dominance (combined score $c = w\cdot j + (1-w)\cdot\text{softcap}(f)$). The goal is not to replicate historical eliminations, but to (i) preserve fan relevance, (ii) improve robustness/predictability in close outcomes, and (iii) reduce judge--fan controversy cases (e.g., Jerry Rice, Billy Ray Cyrus, Bristol Palin, Bobby Bones). Our evaluation reports (a) how often outcomes would change under the proposed rule (scope), (b) controversy-mismatch reduction, and (c) robustness-radius improvements relative to the historical rule.
\end{itemize}
\newpage

% ============================================================
% Table of Contents
% ============================================================
\tableofcontents
\newpage

% ============================================================
% 2) Main Report
% ============================================================
\section{Problem Context and Data}
\subsection{Competition and Rule Regimes}
The show combines judges' scores and fan votes to eliminate the couple with the lowest combined score each week; fans vote to keep a couple (not to eliminate one), and judges score each performance \cite{comap2026}. In the first two U.S. seasons, the combination used \emph{ranks}; beginning in season 3, producers switched to a \emph{percent} method after season 2 concerns, and in response to a season 27 controversy, a judges-save modification (choose which of the bottom two to eliminate) was introduced around season 28 together with a return to the rank-based method \cite{comap2026}. The exact season is not known, but assuming season 28 is reasonable.

\subsection{Assumptions and Notation}
We assume: (i) season 28 is the rule-change point (judges-save and return to rank); (ii) index scaling (e.g., $V_i = f_i \times 10^7$) is a normalization for interpretability only, not true vote counts \cite{comap2026}; (iii) a contestant is ``active'' in a given week if they have not yet been eliminated and have not withdrawn; we use the active set when computing combined scores and elimination. Notation: $A_t$ = active contestants in week $t$; $J_{i,t}$ = total judges score; $f_{i,t}$ = fan vote share; $j_{i,t}$, $c_{i,t}$, $R_{i,t}$ as in the voting schemes below.

\subsection{Dataset and Preprocessing}
\paragraph{Data structure.}
The provided CSV has one row per contestant (celebrity and ballroom partner), columns for \texttt{results} and \texttt{placement}, and wide score columns \texttt{week\{$k$\}\_judge\{$j$\}\_score} for each week and judge. We melt to long form (contestant--week--judge), then aggregate to contestant--week with \texttt{score\_week\_total} = sum of non-missing judge scores and \texttt{num\_judges} = count of non-missing scores.

\paragraph{Definition of ``active.''}
A contestant is \textbf{active} in week $t$ if and only if their \emph{score\_week\_total} for that week is strictly greater than zero. Thus ``active'' means they received at least one nonzero judge score that week; after elimination or withdrawal, scores are typically zero or missing, so they are inactive in later weeks. All vote-combination and elimination logic uses only the active set in that week.

\paragraph{Parsing elimination weeks.}
We assign each contestant an \textbf{elimination\_week} (the week they leave the competition) as follows.
\begin{itemize}[nosep]
  \item \textbf{From results:} We parse the \texttt{results} string with the regex ``Eliminated Week $k$'' (case-insensitive); if matched, \texttt{elimination\_week} = $k$. If the string is missing, empty, or does not match (e.g., ``1st Place,'' ``2nd Place,'' ``Withdrew''), we do not set elimination from results.
  \item \textbf{From scores:} For each contestant we find the first week where \texttt{score\_week\_total} is zero and the previous week had a positive total; that week is \texttt{elimination\_week\_from\_scores}.
  \item We set \texttt{elimination\_week} = result from results when available, else from scores. A week is an \textbf{elimination week} (for a given season) if at least one active contestant has \texttt{elimination\_week} equal to that week (i.e., someone was sent home that week).
\end{itemize}

\paragraph{Edge cases.}
\begin{itemize}[nosep]
  \item \textbf{N/A and missing scores:} In the long-form melt, any cell that is blank, empty, or the literal string ``N/A'' is treated as missing (NaN). Zeros are kept as zero. Contestant--week \texttt{score\_week\_total} is the sum of non-NaN scores; if all judges are N/A for that contestant--week, the total is zero, so the contestant is \emph{inactive} that week.
  \item \textbf{Withdrawals:} If \texttt{results} contains ``Withdrew,'' we do not use ``Eliminated Week $k$'' (there is none). We set \texttt{elimination\_week} = the \emph{last} week in which the contestant had a positive \texttt{score\_week\_total}. Thus withdrawals are treated as leaving after their last scored week; no specific ``elimination week'' in the sense of ``sent home this week'' is assigned, but they are marked eliminated from the following week onward.
  \item \textbf{No-elimination weeks:} Some weeks (e.g., finals, specials) have no contestant with \texttt{elimination\_week} equal to that week. These weeks still appear in the contestant--week panel and count toward the active set, but there is no \emph{observed} eliminated contestant for that week.
  \item \textbf{Double eliminations:} When more than one contestant has \texttt{elimination\_week} equal to the same week, we treat the week as having one observed elimination and take the first such contestant (by row order) for consistency evaluation. The week is still a single elimination week in our counts.
\end{itemize}

\paragraph{265 vs.\ 335 weeks.}
We distinguish two week counts used in the report.
\begin{itemize}[nosep]
  \item \textbf{335 weeks} = \textbf{all eligible forward-event weeks}: every (season, week) in which there are at least two active contestants and we have judge totals and (after fitting) fan shares. Rule comparison (percent vs.\ rank vs.\ judges-save), sensitivity sweeps, and robustness radii are computed over all 335 weeks.
  \item \textbf{265 weeks} = \textbf{weeks with an observed elimination}: the subset of those 335 weeks for which there is at least one contestant with \texttt{elimination\_week} equal to that week. Consistency (fit to eliminations)---how often the model's implied eliminated contestant matches the observed one---is evaluated only on these 265 weeks, because only then do we have a defined ``observed eliminated'' to compare against. The remaining 70 weeks (335 $-$ 265) are eligible (scores and fan shares exist) but are no-elimination or other edge-case weeks, so they are excluded from the inverse-fit consistency metric.
\end{itemize}
Summarizing: 335 = all weeks where we can apply the scoring rules and compare outcomes; 265 = those same weeks where we also observe who was eliminated, used for consistency.

\subsection{Definition audit (paper--code alignment)}
\label{sec:definition_audit}
To avoid definitional mismatches between prose and implementation, we state precisely:

\textbf{Fan influence index (FII).} In this report and in \texttt{season\_rule\_comparison.csv}, the fan influence index is defined as in the code (\texttt{src/analysis/counterfactual\_engine.py}): the \emph{fraction of elimination weeks in which at least one contestant with poor judge score} (bottom half of $J$ in that week) \emph{but high fan share} (top half of $f$) \emph{survives} (is not eliminated) under that rule. So FII answers: ``How often does a poor-judge/high-fan contestant survive?'' An alternative definition---``fraction of weeks where the eliminated couple would change if fan share were neutralized''---is not used here.

\textbf{Sensitivity flips (\texttt{sensitivity\_flip\_summary.csv}).} A \emph{flip} at a given judge weight $w_J$ is defined as: the eliminated contestant at that $w_J$ \emph{differs from the eliminated contestant at the previous grid point} (not vs.\ historical observed elimination). The sweep uses $c_i = w_J\, j_i + (1-w_J)\, f_i$ over a grid of $w_J$; for each (season, week) we record whether the eliminated index changes when moving to the next $w_J$. So \texttt{n\_flips} at $w_J=1.0$ means 26 weeks flip when moving from $w_J=0.98$ to $1.0$; at $w_J=0$ there is no previous grid point, so flips are not defined there. We use this definition consistently in the report.

\subsection{Output map}
All tables produced by \texttt{python main.py} appear in \texttt{reports/tables/}\ldots

\section{Voting Schemes: Rank, Percent, and Judges-Save}
Let $A_t$ be active contestants in week $t$ and $J_{i,t}$ the total judges score.
\subsection{Percent scheme}
Define judges percent $j_{i,t}=J_{i,t}/\sum_{k\in A_t}J_{k,t}$ and fan percent $f_{i,t}=V_{i,t}/\sum_{k\in A_t}V_{k,t}$. Combined score $c_{i,t}=j_{i,t}+f_{i,t}$; eliminated is $\argmin_i c_{i,t}$.
\subsection{Rank scheme}
Let $r^J_{i,t}$ be rank of $J_{i,t}$ (best=1) and $r^F_{i,t}$ rank of $V_{i,t}$; combined $R_{i,t}=r^J_{i,t}+r^F_{i,t}$; eliminated is $\argmax_i R_{i,t}$.
\subsection{Judges-save modification}
Among the bottom two by combined criterion, judges select which couple to eliminate; we model this probabilistically with
\[
\Pr(\text{eliminate }i\mid\{i,k\})=\sigma\bigl(\alpha(J_{k,t}-J_{i,t})\bigr),
\]
and fit $\alpha$ from observed outcomes in the judges-save era. Fitted $\alpha\approx 0.029$ (from \texttt{judges\_save\_alpha.json}).

\subsection{Rule simulators}
Our code implements these rules as deterministic simulators for counterfactuals and consistency checks.

\paragraph{Percent vs.\ rank tie-breaking.}
\begin{itemize}[nosep]
  \item \textbf{Percent:} Combined score $c_i = j_i + f_i$; eliminated = $\argmin_i c_i$. Ties (multiple contestants with the same minimum $c$) are broken by \emph{index order}: we take the first index achieving the minimum (e.g., \texttt{np.argmin}, which returns the first occurrence). So the contestant who appears first in the active list among those tied for worst is eliminated.
  \item \textbf{Rank:} Judge ranks $r^J_i$ and fan ranks $r^F_i$ use \emph{average} ranks when raw scores tie (e.g., \texttt{scipy.stats.rankdata} with \texttt{method='average'}). Combined $R_i = r^J_i + r^F_i$; eliminated = $\argmax_i R_i$ (worst rank-sum). Tie-breaking for ``who is eliminated'' is again by index order: the first index achieving the maximum is eliminated.
\end{itemize}
So in both schemes, a tie on the combined metric is resolved deterministically by choosing the contestant with the smaller index in the active set.

\paragraph{Judges-save bottom-two logic.}
In seasons that use judges save (we assume from season 28 onward), the simulator does not eliminate by combined score alone. Steps:
\begin{enumerate}[nosep]
  \item \textbf{Bottom two:} Identify the two contestants with the \emph{worst} combined score. For the rank rule, $R_i$ is the rank-sum (higher = worse), so the bottom two are the two with \emph{largest} $R$. Ties (e.g., who is ``second-worst'') are broken by index order: we sort by combined score and take the last two indices (stable argsort), so the two worst are uniquely chosen.
  \item \textbf{Judges' choice:} Among these two, the contestant with the \emph{lower} judges' total $J$ is eliminated; the one with the higher $J$ is ``saved.'' Thus judges save the higher-scoring of the bottom two. If $J$ is tied for the two, we break the tie by index (the second of the two indices is eliminated in our implementation).
\end{enumerate}
The probabilistic model $\Pr(\text{eliminate }i\mid\{i,k\})=\sigma(\alpha(J_k-J_i))$ is used only for \emph{fitting} $\alpha$ from observed outcomes; the rule simulator used for counterfactuals and consistency is deterministic (lower $J$ among bottom two $\to$ eliminated).

\section{Model for Latent Fan Vote Shares}
\subsection{Identifiability}
Under the percent rule, only \emph{shares} $f_{i,t}=V_{i,t}/\sum_{k\in A_t}V_{k,t}$ are identified: vote totals can be scaled arbitrarily without changing combined scores $c_{i,t}=j_{i,t}+f_{i,t}$. Under the rank rule, only the \emph{ordering} of vote totals is identified. We therefore report fan vote \emph{shares} as primary outputs and provide index-scaled totals (e.g., $V_i = f_i \times 10^7$) only for interpretability, with a clear label that they are not true vote counts. This non-identifiability is also illustrated in the problem appendix, which notes that many hypothetical fan vote totals can reproduce the same eliminations and presents an example using an arbitrary total of 10 million votes \cite{comap2026}.

\subsection{Latent preference model}
Let $A_t$ denote the set of active contestants in week $t$. We model fan share as a multinomial logit (softmax) over a latent utility $u_{i,t}$:
\[
f_{i,t} = \frac{\exp(u_{i,t})}{\sum_{k\in A_t}\exp(u_{k,t})},
\]
with
\[
u_{i,t} = \beta_0 + \beta_J\,\tilde J_{i,t} + \beta_P\,\text{momentum}_{i,t} + \beta_U\,\text{underdog}_{i,t} + \beta_X^\top X_i.
\]
Terms are defined as follows. \textbf{Normalized judge score} $\tilde J_{i,t}$: within each $(season, week)$, we set $\tilde J_{i,t} = J_{i,t} / \max_{k\in A_t} J_{k,t}$ (and 0 if the max is 0), so that the best-scoring contestant has $\tilde J=1$. This captures the extent to which judges favor contestant $i$ relative to the field. \textbf{Momentum} (denoted $p_{\text{prev}}$ in code): rank by judges' score in the previous week (1 = best); for week 1 we set it to 0. Higher rank last week may attract more fan attention (momentum) or, if negative, an ``underdog'' effect. \textbf{Underdog}: we set $\text{underdog}_{i,t}=1$ if $J_{i,t}$ is at or below the median judges score among active contestants in that week, else 0. This allows a ``rage vote'' or sympathy effect for lower-scoring contestants. \textbf{Covariates} $X_i$: we include celebrity age (during the season) and an industry dummy (e.g., 1 if Actor/Actress). The coefficients $\beta = (\beta_0,\beta_J,\beta_P,\beta_U,\beta_X)$ are shared across seasons and weeks; the season-appropriate rule (percent or rank) is applied when mapping $(J,f)$ to combined scores and thus to elimination.

\subsection{Likelihood and fitting}
We fit $\beta$ and a temperature $\tau>0$ by maximum likelihood. The likelihood has two parts.

\textbf{Elimination events.} For each week with an elimination, let $c_{i,t}$ (percent) or $R_{i,t}$ (rank) be the combined score under the rule for that season. We model the probability that contestant $i$ is eliminated as a softmax over the active set. Under percent: lower combined score $c$ implies higher elimination probability; we set
\[
\Pr(\text{eliminate }i) \propto \exp(-\tau\, c_{i,t}).
\]
Under rank: higher rank-sum $R$ (worse) implies higher elimination probability; we set
\[
\Pr(\text{eliminate }i) \propto \exp(\tau\, R_{i,t}).
\]
Thus $\tau$ controls determinism: large $\tau$ makes the lowest $c$ (or highest $R$) almost surely eliminated; small $\tau$ flattens the distribution.

\textbf{Finals ordering.} For each season's finals week, we observe the placement order (1st, 2nd, 3rd). We model this with a Plackett--Luce likelihood: the probability of the observed ordering given strengths $s_i$ (we use combined score for percent, or negative rank-sum for rank so that better rank-sum gives higher strength) is the product over positions of the probability of choosing that contestant from the remaining set, with choice probabilities proportional to $\exp(s_i)$.

The total log-likelihood is the sum of log-probabilities over all elimination events plus the sum over all finals events. We minimize the \emph{negative} log-likelihood with respect to $(\beta,\tau)$ using L-BFGS-B, with $\tau$ bounded below by a small positive constant. Covariates are built from the contestant--week data (normalized judge score, previous-week rank, underdog, age, industry dummy); see \texttt{src/models/vote\_latent.py} and \texttt{src/fit/fit\_elimination.py}. Fit diagnostics (elimination match rates, finals likelihood) are reported in Section~\ref{sec:fitquality}.

\section{Uncertainty Quantification}
We measure uncertainty in two complementary ways.

\textbf{(1) Bootstrap intervals for fan shares.} We resample \emph{seasons} with replacement (e.g., 50 bootstrap replicates). For each replicate we refit $(\beta,\tau)$ on the resampled contestant--week data, then run the forward pass on the \emph{full} dataset with the fitted $\beta$ to obtain fan shares $f_{i,t}$ for every $(season, week, contestant)$. Across replicates we compute the mean, 5th percentile, and 95th percentile of $f_{i,t}$ at each cell. This yields interval estimates for $f_{i,t}$ that reflect uncertainty due to season-to-season variation in the elimination and finals data. Optionally, we can bootstrap by resampling \emph{weeks} within each season instead of seasons; the implementation supports both (see \texttt{src/fit/uncertainty.py}).

\textbf{(2) Margin-to-flip robustness radius.} For each elimination week we ask: how much must fan shares $f_t$ change so that a \emph{different} contestant would be eliminated under the same rule? We parameterize perturbations in log-share space: $f' = \mathrm{softmax}(\log f + z)$ with $z$ unconstrained, so that $f'$ remains on the simplex. We find the minimum L2 norm of $z$ such that the eliminated contestant under the week's rule (percent: $\argmin_i (j_i + f'_i)$; rank: $\argmax_i (r^J_i + r^F_i)$ with ranks from $f'$) is not the currently eliminated contestant. The optimization is a constrained nonlinear problem (minimize $\|z\|^2$ subject to the flip constraint); we use SLSQP. For interpretability we also report the L2 norm of $(f'-f)$ in share space as the ``robustness radius.'' A \emph{small} radius means the outcome is sensitive to small changes in fan shares (uncertain week); a \emph{large} or infinite radius means the eliminated contestant would not change under plausible perturbations (certain week). See \texttt{src/fit/margin\_to\_flip.py}.

\textbf{Examples.} Tight week: Season~10 Week~1, robustness radius $0$ (outcome sensitive to small changes in fan shares). Blowout week: Season~12 Week~3, robustness radius $\infty$ (no feasible perturbation changes the eliminated contestant; elimination is robust). Certainty varies by week and contestant because of the geometry of combined scores near the elimination boundary: when the bottom two are close, a small shift in $f$ can flip who is eliminated; when one contestant is clearly last, the radius is large or infinite.

Table~\ref{tab:robustness_summary} shows the distribution of margin-to-flip robustness radii under the proposed scoring rule (weighted saturation) across all 335 elimination weeks.
About \textbf{8.7\%} of elimination weeks are knife-edge (robustness radius $0$), meaning arbitrarily small perturbations to fan shares can flip who goes home,
while \textbf{14.9\%} are blowouts (radius $\infty$), meaning no feasible perturbation flips the eliminated couple under the rule.
Among finite-radius weeks, the median radius is \textbf{1.36} (IQR \textbf{1.00--1.77}), indicating that most weeks require a nontrivial shift in the fan-share simplex to change the outcome.

\begin{table}[H]
\centering
\caption{Distribution of margin-to-flip robustness radii under the proposed scoring rule (weighted saturation). Radii are computed per (season, week) elimination event; larger means more robust.}
\label{tab:robustness_summary}
\begin{tabular}{lr}
\toprule
Quantity & Value\\
\midrule
Number of elimination weeks & 335\\
Zero-radius (knife-edge) weeks & 29 (8.7\%)\\
Infinite-radius (blowout) weeks & 50 (14.9\%)\\
\midrule
Finite-radius median & 1.36\\
Finite-radius IQR & 1.00 -- 1.77\\
Finite-radius max & 3.05\\
\bottomrule
\end{tabular}
\end{table}

\section{Rank vs Percent Across Seasons}

\subsection{Across-season disagreement rates and fan influence}

Table~\ref{tab:rule_compare_stats} summarizes how often different elimination rules disagree about who goes home.
Across 34 seasons, percent and rank rules disagree in \textbf{0.0\%--36.4\%} of elimination weeks (mean \textbf{13.4\%}, median \textbf{10.6\%}).
Six seasons have \emph{zero} disagreement (seasons 3, 4, 10, 17, 20, 26), while the largest disagreement occurs in season~32 (36.4\%).
Table~\ref{tab:top_disagree_seasons} lists the highest-disagreement seasons.

Judges-save (a bottom-two selection rule) can change outcomes more frequently: percent vs.\ judges-save differs by \textbf{0.0\%--50.0\%} (mean \textbf{22.3\%}),
and rank vs.\ judges-save differs by \textbf{0.0\%--54.5\%} (mean \textbf{17.1\%}).
This is consistent with judges-save acting as a discretionary override precisely in weeks where the bottom of the leaderboard is crowded.

We also compute a \emph{fan influence index} (FII), defined in Section~\ref{sec:definition_audit}: the fraction of elimination weeks in which at least one contestant with poor judge score (bottom half) but high fan share (top half) \emph{survives} (is not eliminated) under that rule.
FII is high in most seasons: mean \textbf{0.917} under percent and \textbf{0.900} under rank, with values ranging from \textbf{0.333} to \textbf{1.000}.
In other words, in a typical season, a poor-judge/high-fan contestant survives in the majority of weeks, so fan influence is substantial under both percent and rank.

\begin{table}[H]
\centering
\caption{Across-season disagreement between elimination rules and fan influence. Each season contributes its fraction of elimination weeks where the eliminated couple differs under the two rules. Fan influence index (FII) is the fraction of weeks a poor-judge/high-fan contestant survives (Section~\ref{sec:definition_audit}).}
\label{tab:rule_compare_stats}
\begin{tabular}{lcccc}
\toprule
Statistic & pct vs rank & pct vs save & rank vs save & fan infl.\ (pct / rank)\\
\midrule
Min & 0.0\% & 0.0\% & 0.0\% & 0.333 / 0.333\\
Median & 10.6\% & 22.2\% & 19.1\% & 0.909 / 0.909\\
Mean & 13.4\% & 22.3\% & 17.1\% & 0.917 / 0.900\\
Max & 36.4\% & 50.0\% & 54.5\% & 1.000 / 1.000\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Seasons with the largest disagreement between percent and rank elimination outcomes.}
\label{tab:top_disagree_seasons}
\begin{tabular}{rrrr}
\toprule
Season & Elim.\ weeks & pct vs rank diff & fan infl.\ (pct / rank)\\
\midrule
32 & 11 & 36.4\% & 1.000 / 1.000\\
11 & 10 & 30.0\% & 1.000 / 1.000\\
19 & 11 & 27.3\% & 1.000 / 0.909\\
21 & 11 & 27.3\% & 0.909 / 0.909\\
\bottomrule
\end{tabular}
\end{table}

From \texttt{sensitivity\_flip\_summary.csv}: varying judge weight from 0 to 1, the fraction of weeks where elimination would flip is highest at weight 0 (26/335 weeks) and 0 at weight 1; intermediate weights show modest flip rates (e.g., 2--9 flips at 0.76--0.86).

\section{Controversy Case Studies}
We examine the prompt's highlighted controversy examples \cite{comap2026}: Season 2 (Jerry Rice), Season 4 (Billy Ray Cyrus), Season 11 (Bristol Palin), and Season 27 (Bobby Bones). For each, we compare rank vs.\ percent outcomes and quantify how a judges-save rule would alter eliminations when the bottom two are close. Data: \texttt{controversy\_season*\_fan\_shares\_vs\_judges.csv} (judge vs.\ inferred fan disagreement) and \texttt{controversy\_season*\_counterfactual\_elimination.csv} (counterfactual eliminations under judges-save).

\section{Drivers of Performance: Pro Dancer and Celebrity Characteristics}
\label{sec:drivers}

We estimate two parallel linear models: one for judges outcomes and one for inferred fan outcomes, using the same core covariates.
Table~\ref{tab:judges_fans_covars} compares the common covariates side-by-side.

Two effects are strong and consistent across judges and fans:
\textbf{week} has a negative coefficient in both models, and \textbf{age} is strongly negative in both models.
Industry and region behave differently: \textbf{industry\_dummy} is positive and highly significant for judges but near zero and not significant for fans,
suggesting judges respond to industry-related factors more than the voting public does.
The \textbf{region\_us} indicator is modestly negative and significant for judges but not significant for fans.

(Separately, pro-dancer fixed effects provide the largest heterogeneity in the cross-sectional fit; we summarize those in \texttt{pro\_dancer\_effects\_top10.csv} and \texttt{pro\_dancer\_effects\_bottom10.csv}. Pros with largest positive ``fan minus judge'' effect: Henry Byalikov, Andrea Hale; judges-favoring: Koko Iwasaki, Ashly DelGrosso.)

\begin{table}[H]
\centering
\caption{Common-covariate comparison for judges vs.\ fans regressions. OLS coefficients and $p$-values; stars correspond to conventional significance levels.}
\label{tab:judges_fans_covars}
\begin{tabular}{lrrrr}
\toprule
Covariate & Judges coef.\ & $p$-value & Fans coef.\ & $p$-value\\
\midrule
week & $-0.042$ *** & $1.21\times10^{-11}$ & $-0.046$ *** & $2.22\times10^{-15}$\\
age & $-0.033$ *** & $3.38\times10^{-91}$ & $-0.048$ *** & $2.83\times10^{-172}$\\
industry\_dummy & $0.183$ *** & $3.05\times10^{-6}$ & $0.010$ & $0.79$\\
region\_us & $-0.134$ ** & $1.29\times10^{-2}$ & $-0.081$ & $0.11$\\
\bottomrule
\end{tabular}
\end{table}

\section{Proposed ``Better'' System and Evaluation}
We propose a \textbf{weighted percent with saturation} and an optional trigger-based judges-save. The combined score is $c_i = w\cdot j_i + (1-w)\cdot\text{softcap}(f_i)$; the softcap is applied to \emph{fan} share to prevent extreme fan-bloc dominance (not judge dominance). Optionally: trigger judges-save when the bottom-two margin is below a threshold. Fairness axioms: monotonicity (better combined score $\Rightarrow$ not eliminated), fan relevance bounds, robustness (margin-to-flip), transparency. Evaluation: we report (a) scope of change---how often the proposed rule would yield a different eliminated contestant than observed (see \texttt{proposed\_system\_eval.csv}); (b) controversy-mismatch reduction; (c) robustness radius by week (Table~\ref{tab:robustness_summary}). Operationally, the robustness radius provides a transparent ``closeness'' signal: in our data, about 8.7\% of weeks are knife-edge (radius $0$), suggesting a judges-save trigger should be reserved for a small subset of genuinely close eliminations rather than used routinely.

\section{Fit Quality and Validation}
\label{sec:fitquality}
\begin{itemize}[leftmargin=*]
  \item \textbf{Elimination prediction (consistency):} Using the \emph{fitted latent fan-share model}, the implied (MAP) eliminated contestant matches the observed in 38.2\% of percent-weeks (76/199) and 30.3\% of rank-weeks (20/66), vs.\ random baseline $\approx 17\%$ when $\sim 6$ contestants. Mean log probability assigned to the observed eliminated is $\approx -1.94$; in 37.7\% of weeks the observed eliminated was rank 1, in 60.8\% rank $\le 2$, in 74.3\% rank $\le 3$. Inverse-fit sample: 265 elimination weeks; full universe 335 weeks \cite{comap2026}. See \texttt{fit\_diagnostics.csv}, \texttt{fitted\_params.json}, \texttt{reports/gonogo\_report.md}.
  \item \textbf{Baselines:} We report two baselines in \texttt{fit\_diagnostics.csv} beside the model: (1) \emph{random elimination among active} (Pr(elim$=i$)$=1/n$); (2) \emph{judges-only}---percent-era: combined score $c_i=j_i$ (fan share uniform); rank-era: $R_i=r^J_i$ (fan rank uniform). The table gives per-week baseline match (random: expected match prob $1/n$; judges-only: MAP match) and log probability assigned to the observed eliminated. The model beats both baselines on mean log probability (higher is better). On percent-era weeks the model match rate exceeds the judges-only match rate, a clean win for the latent fan-share model.
  \item \textbf{Finals likelihood:} Plackett--Luce term is non-degenerate; ordering likelihood contributes to identification.
  \item \textbf{Robustness:} Margin-to-flip shows clear variation (Table~\ref{tab:robustness_summary}): 29 knife-edge weeks (8.7\%), 50 blowout weeks (14.9\%), finite-radius median 1.36; see \texttt{proposed\_system\_robustness.csv}.
\end{itemize}

\section{Limitations and Extensions}
Identifiability: only shares (percent) or order (rank) are identified. Regime uncertainty: season~28 judges-save start is assumed. Unobserved confounders: marketing, social media, contestant visibility. Extensions: incorporate viewership or social sentiment if data become available.

% ============================================================
% Appendix (optional): Mean-field / network interpretation
% ============================================================
\appendix
\section{Mechanistic Interpretation via Biased Mean-Field Voter Dynamics (Optional)}
\label{sec:meanfield}

The mean-field/memory/network module (in \texttt{src/models/meanfield.py}) is used for \emph{interpretation and robustness simulation}, not as the primary inference engine. It provides a mechanistic story for how judge scores and social influence could generate vote shares over time; we treat the $\beta$-fitted latent vote model in \texttt{main.py} as the primary estimator. Below we summarize the structure so that results from this module can be interpreted consistently with the main report.

\subsection{Role relative to the primary model}
The primary model (Section~3) fits a single set of coefficients $\beta$ and temperature $\tau$ by maximum likelihood so that implied eliminations match observed outcomes. That model is \emph{static} in the sense that fan share in week $t$ depends on covariates and judge score in week $t$ (and optionally momentum/underdog from the same or previous week), but there is no explicit dynamical law linking $p_t$ to $p_{t-1}$. The mean-field module adds a \emph{discrete-time dynamical} layer: fan shares evolve week-to-week via a recurrence $p_{t+1} = \Phi(p_t, S_t, \ldots)$. This can be used to (i) interpret how judge signals and past popularity might combine into a plausible evolution of votes, and (ii) run robustness or scenario simulations (e.g., different memory kernels or network coupling) without re-fitting the primary $\beta$.

\subsection{Single-population mean-field update (F2)}
The core recurrence is
\[
p_{t+1} = (1 - \kappa)\, p_t + \kappa\, \mathrm{softmax}(u_t), \qquad u_t = \eta\, S_t + \gamma\, \log(p_t + \varepsilon) + \text{(optional terms)}.
\]
Here $p_t$ is the vector of fan shares (simplex), $S_t$ is the vector of judge signals (e.g., normalized scores) in week $t$, and $\kappa \in (0,1]$ is a mixing speed: the new share is a convex combination of the previous share and a softmax of the utility $u_t$. The term $\gamma\, \log(p_t)$ captures \emph{incumbency} or social reinforcement (higher current share $\Rightarrow$ higher utility, all else equal). The term $\eta\, S_t$ is the judge-driven component. Optional terms include covariates $\theta^\top X$, an underdog term $\beta_U\,\text{underdog}_t$, and memory terms described below.

\subsection{Switching-rate microfoundation (F1)}
One can motivate the update by a \emph{switching-rate} story: voters are drawn toward a target distribution $q_t$ that mixes current popularity and a judge-driven distribution. For example, $q_t = \rho\, p_t + (1-\rho)\,\mathrm{softmax}(\eta\, S_t)$, normalized. Then $p_{t+1} = (1-\kappa)\, p_t + \kappa\, q_t$ corresponds to a mean-field law where a fraction $\kappa$ of the population ``switches'' toward $q_t$ each period. The implementation supports this via the same recurrence with $u_t$ constructed so that $\mathrm{softmax}(u_t)$ matches the desired target (e.g., with $\gamma$ and $\eta$ playing the roles of $\rho$ and judge weight).

\subsection{Underdog / rage-vote (F3)}
The underdog score is a scalar per contestant that is high when the contestant has \emph{low} judge score but \emph{high} current popularity (or vice versa, depending on parameterization). Formally, $\text{underdog}_i = \sigma(a(p_i - \tau_p))\,\sigma(b(\tau_S - S_i))$ in smooth mode, or an indicator $1[S_i \le \tau_S]\, 1[p_i \ge \tau_p]$ in indicator mode, with thresholds $\tau_S$, $\tau_p$ (e.g., medians). This is added to $u_t$ as $\beta_U\,\text{underdog}_t$, so that ``underdog'' contestants get a utility boost and can sustain higher share despite lower judge scores---a simple model of sympathy or rage voting.

\subsection{Memory: kernel-weighted history and Markovian state}
Two types of memory are supported. (1) \textbf{Kernel-weighted history:} define $\bar{S}_t = \sum_{\tau=0}^t k(t-\tau)\, S_\tau$ and $\bar{p}_t$ similarly, where $k(\Delta t)$ is a kernel (e.g., exponential $k(\Delta t) = \lambda^{\Delta t}$, rectangular window, or power-law). Then $u_t$ can include $\eta_S\, \bar{S}_t$ and $\gamma_{\text{hist}}\, \log(\bar{p}_t + \varepsilon)$, so that past judge signals and past popularity influence current utility. (2) \textbf{Markovian fading state:} a single state vector $m_t$ is updated by $m_{t+1} = (1-\lambda)\, m_t + \lambda\, S_t$, and $u_t$ includes $\eta_m\, m_t$. This gives a one-dimensional fading memory of judge signals. Both channels are optional; when omitted, the model reduces to the static-like update with only $S_t$ and $\log(p_t)$.

\subsection{Networked extension: multiple communities and small-world coupling}
The code also supports $G$ ``communities'' (e.g., demographic or regional voter blocs), each with its own share vector $p_t^{(g)}$ on the simplex. Communities are coupled via a row-stochastic influence matrix $W$ (e.g., from a Watts--Strogatz small-world graph: ring lattice with $K$ neighbors, then each edge rewired with probability $\beta$; then $W$ is adjacency plus self-weight $\omega_{\mathrm{self}}$, row-normalized). The utility in community $g$ is
\[
u_t^{(g)} = \eta\, S_t + \gamma\, \log(p_t^{(g)} + \varepsilon) + \delta\, (W \log(p_t + \varepsilon))_g + \text{(covariates, underdog)}.
\]
The term $(W \log p_t)_g$ is the weighted average of $\log p_t^{(h)}$ over neighbors $h$ of $g$, so that high popularity in neighboring communities boosts utility in $g$ (social influence across blocs). The aggregate share reported for elimination can be $\bar{p}_t = \sum_g w_g\, p_t^{(g)}$ for reporting weights $w_g$. Optional logit-normal noise ($\sigma_{\mathrm{shock}}$) can be added to $u$ before softmax for robustness checks.

\subsection{Parameters and use in the report}
Key parameters: $\kappa$ (mixing speed), $\eta$ (judge weight), $\gamma$ (incumbency/social weight), $\delta$ (cross-community weight), $\beta_U$ (underdog weight), and memory parameters (kernel type, decay/window, $\eta_S$, $\gamma_{\text{hist}}$, $\lambda$, $\eta_m$). These are \emph{not} fit by the same MLE as the primary model; they can be set for scenario or sensitivity analysis. In the main report we do not report point estimates from the mean-field module; we use it only to interpret how dynamics and memory could generate patterns consistent with the primary $\beta$-fit and to run robustness simulations (e.g., different kernels or coupling strength) when needed.

% ============================================================
% 3) Memo (1–2 pages)
% ============================================================
\newpage
\section*{Memo to DWTS Producers (1--2 pages)}
\textbf{To:} Producers of \DWTS \\
\textbf{From:} Team \#\_\_\_\_ \\
\textbf{Subject:} Recommended method for combining judges and fan votes \\
\vspace{0.5em}

\textbf{Executive recommendation.} We recommend adopting a \textbf{weighted percent rule with saturation} and an \textbf{optional judges-save trigger} in close weeks. The saturation softcaps \emph{fan} share to prevent extreme fan-bloc dominance (combined score $c = w\cdot j + (1-w)\cdot\text{softcap}(f)$), keeping fan votes meaningful without letting a single fan bloc dominate. The data show that percent and rank disagree on who goes home in only about 13\% of elimination weeks on average (range 0--36\% by season), and fan influence is already high (index 0.33--1.0, mean 0.92). The prompt's controversy examples---Season 2 (Jerry Rice), Season 4 (Billy Ray Cyrus), Season 11 (Bristol Palin), Season 27 (Bobby Bones)---show clear judge--fan disagreement; a judges-save can mitigate those cases \cite{comap2026}.

\textbf{Evidence.}
\begin{itemize}[leftmargin=*]
  \item \textbf{Rule comparison (season\_rule\_comparison.csv):} Across 34 seasons, percent vs.\ rank disagree on who is eliminated in \textbf{0--36.4\%} of elimination weeks by season (mean \textbf{13.4\%}). Fan-influence index is \textbf{0.33--1.0} (mean 0.92) for both rules---fans already have substantial weight.
  \item \textbf{Consistency (fitted latent model):} Fitted $\beta$ and $\tau$ are in \texttt{fitted\_params.json}. Using our \emph{fitted latent fan-share model}, the MAP eliminated matches the observed in \textbf{38.2\%} of percent-weeks (76/199) and \textbf{30.3\%} of rank-weeks (20/66), well above random baseline $\approx 17\%$. Mean log probability assigned to the observed eliminated is $\approx -1.94$; in 37.7\% of weeks the observed eliminated was rank 1, in 60.8\% rank $\le 2$, in 74.3\% rank $\le 3$. Baselines (random and judges-only) are in \texttt{fit\_diagnostics.csv}; the model beats judges-only on percent-era weeks. Inverse-fit sample: 265 elimination weeks; full universe 335 weeks \cite{comap2026}. See \texttt{fit\_diagnostics.csv}, \texttt{fitted\_params.json}, \texttt{reports/gonogo\_report.md}.
  \item \textbf{Uncertainty and robustness:} Certainty varies by week. Season~10 Week~1 has robustness radius 0 (tight); Season~12 Week~3 has radius infinite (blowout). Our proposed rule evaluation reports robustness-radius improvements and controversy-mismatch reduction relative to the historical rule; we report the \emph{scope} of change (how often the proposed rule would yield a different elimination) separately from consistency.
  \item \textbf{Controversy seasons:} In seasons 2, 4, 11, and 27 (Jerry Rice, Billy Ray Cyrus, Bristol Palin, Bobby Bones), inferred fan shares often disagree with judge rankings. A judges-save option lets producers avoid outcomes that would outrage fans when the bottom two are close.
  \item \textbf{Pro/celebrity effects (pro\_dancer\_effects\_top10.csv):} Pros who boost \emph{fans} relative to judges: \textbf{Andrea Hale} (2.03), \textbf{Henry Byalikov} (1.01). Judges-favoring pros appear in the bottom of the effects table. A stable rule keeps competition fair across pros.
\end{itemize}

\textbf{Proposed system.} Combine judge and fan contributions as $c = w\cdot j + (1-w)\cdot\text{softcap}(f)$; the softcap is on \emph{fan} share to prevent extreme fan-bloc dominance. Optionally: when the bottom two are within a small margin, trigger a judges-save (we fit $\alpha\approx 0.029$ from season~28+ data). This preserves fan relevance, satisfies monotonicity and transparency, and reduces controversy in close weeks.

\textbf{Expected impact.} (i) Fan influence remains high (fan-influence index 0.33--1.0, mean 0.92). (ii) Fairness improves via monotonicity and bounded fan-bloc dominance. (iii) Robustness: margin-to-flip analysis identifies which weeks are close; judges-save can be used only in those weeks. (iv) Controversy frequency can decrease when judges-save is triggered in disputed bottom-two situations.

% ============================================================
% References
% ============================================================
\newpage
\begin{thebibliography}{9}
\bibitem{comap2026}
COMAP, \emph{2026 MCM Problem C: Data With The Stars}, 2026.
% Add any other documented sources if you used them.
\end{thebibliography}

% ============================================================
% AI Use Report (not counted in 25 pages)
% ============================================================
\newpage
\section*{AI Use Report}
Describe exactly how AI was used (e.g., brainstorming model structure, code review, writing assistance), what was verified independently (tests, reproducibility checks), and what was not used (no external data unless documented).

\end{document}
